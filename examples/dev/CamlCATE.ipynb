{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caml API Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable\n",
    "\n",
    "df_backend = \"pandas\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08/24/24 19:16:28] </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> Logging has been set up.                                                 <a href=\"file:///home/jakep/projects/caml/caml/logging.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">logging.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/logging.py#42\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08/24/24 19:16:28]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Logging has been set up.                                                 \u001b]8;id=253399;file:///home/jakep/projects/caml/caml/logging.py\u001b\\\u001b[2mlogging.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=43151;file:///home/jakep/projects/caml/caml/logging.py#42\u001b\\\u001b[2m42\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Propensity score is close to 0 or 1. Trimming is at 0.01 and 0.99 is applied\n"
     ]
    }
   ],
   "source": [
    "from caml.extensions.synthetic_data import make_fully_hetereogenous_dataset\n",
    "\n",
    "df, true_cates, true_ate = make_fully_hetereogenous_dataset(\n",
    "    n_obs=1000,\n",
    "    n_confounders=50,\n",
    "    ate=17.2,\n",
    "    seed=None,\n",
    ")\n",
    "\n",
    "df[\"true_cates\"] = true_cates\n",
    "df[\"uuid\"] = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "except ImportError:\n",
    "    pass\n",
    "\n",
    "if df_backend == \"polars\":\n",
    "    df = pl.from_pandas(df)\n",
    "    spark = None\n",
    "elif df_backend == \"pandas\":\n",
    "    spark = None\n",
    "    pass\n",
    "elif df_backend == \"pyspark\":\n",
    "    spark = (\n",
    "        SparkSession.builder.master(\"local[1]\")\n",
    "        .appName(\"local-tests\")\n",
    "        .config(\"spark.executor.cores\", \"1\")\n",
    "        .config(\"spark.executor.instances\", \"1\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"1\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    df = spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X45</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>true_cates</th>\n",
       "      <th>uuid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.246181</td>\n",
       "      <td>-1.274200</td>\n",
       "      <td>-0.950982</td>\n",
       "      <td>-0.705522</td>\n",
       "      <td>0.132961</td>\n",
       "      <td>0.814546</td>\n",
       "      <td>1.883581</td>\n",
       "      <td>-0.188843</td>\n",
       "      <td>0.632661</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.612764</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.113842</td>\n",
       "      <td>0.095730</td>\n",
       "      <td>0.716684</td>\n",
       "      <td>-0.302220</td>\n",
       "      <td>22.787988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.738300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.948685</td>\n",
       "      <td>0.261073</td>\n",
       "      <td>-2.629703</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>-0.352464</td>\n",
       "      <td>1.072183</td>\n",
       "      <td>-1.093752</td>\n",
       "      <td>-0.427244</td>\n",
       "      <td>-2.305874</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>...</td>\n",
       "      <td>1.201444</td>\n",
       "      <td>0.412904</td>\n",
       "      <td>-0.120547</td>\n",
       "      <td>-1.460580</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.445292</td>\n",
       "      <td>4.540934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.616198</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615300</td>\n",
       "      <td>1.659502</td>\n",
       "      <td>-0.099698</td>\n",
       "      <td>0.817223</td>\n",
       "      <td>0.106429</td>\n",
       "      <td>0.114220</td>\n",
       "      <td>0.564847</td>\n",
       "      <td>0.073989</td>\n",
       "      <td>-0.303836</td>\n",
       "      <td>1.339558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.538605</td>\n",
       "      <td>-1.870271</td>\n",
       "      <td>1.196852</td>\n",
       "      <td>-2.239998</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>-1.889548</td>\n",
       "      <td>2.802615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.294353</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887987</td>\n",
       "      <td>-0.017229</td>\n",
       "      <td>1.593608</td>\n",
       "      <td>0.885682</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1.043431</td>\n",
       "      <td>0.394262</td>\n",
       "      <td>-1.698353</td>\n",
       "      <td>0.558425</td>\n",
       "      <td>0.933242</td>\n",
       "      <td>...</td>\n",
       "      <td>1.074709</td>\n",
       "      <td>-0.562967</td>\n",
       "      <td>-0.608498</td>\n",
       "      <td>-1.515747</td>\n",
       "      <td>0.491019</td>\n",
       "      <td>-0.125751</td>\n",
       "      <td>26.132472</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.358482</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.457293</td>\n",
       "      <td>-0.296993</td>\n",
       "      <td>-0.979472</td>\n",
       "      <td>0.810209</td>\n",
       "      <td>-1.433864</td>\n",
       "      <td>-1.018180</td>\n",
       "      <td>-0.199094</td>\n",
       "      <td>-0.996772</td>\n",
       "      <td>-0.402186</td>\n",
       "      <td>2.204502</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.121076</td>\n",
       "      <td>0.549883</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>-0.636527</td>\n",
       "      <td>-0.490393</td>\n",
       "      <td>-1.062060</td>\n",
       "      <td>-3.520146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.477713</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.147637</td>\n",
       "      <td>-0.955530</td>\n",
       "      <td>0.393667</td>\n",
       "      <td>-0.281182</td>\n",
       "      <td>0.626307</td>\n",
       "      <td>-0.422972</td>\n",
       "      <td>-2.630214</td>\n",
       "      <td>0.163528</td>\n",
       "      <td>1.020511</td>\n",
       "      <td>-0.364770</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145645</td>\n",
       "      <td>1.335647</td>\n",
       "      <td>-0.485012</td>\n",
       "      <td>0.668727</td>\n",
       "      <td>-1.590170</td>\n",
       "      <td>1.036452</td>\n",
       "      <td>30.132903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.992916</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.310743</td>\n",
       "      <td>1.540612</td>\n",
       "      <td>0.637802</td>\n",
       "      <td>0.060178</td>\n",
       "      <td>0.820293</td>\n",
       "      <td>0.767454</td>\n",
       "      <td>0.222449</td>\n",
       "      <td>-0.112176</td>\n",
       "      <td>-0.966916</td>\n",
       "      <td>0.391996</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.277102</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.061312</td>\n",
       "      <td>-0.808946</td>\n",
       "      <td>0.333095</td>\n",
       "      <td>1.076270</td>\n",
       "      <td>2.968386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.238688</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.552284</td>\n",
       "      <td>0.074332</td>\n",
       "      <td>0.477756</td>\n",
       "      <td>0.984183</td>\n",
       "      <td>-0.469270</td>\n",
       "      <td>-1.480282</td>\n",
       "      <td>1.335541</td>\n",
       "      <td>1.114310</td>\n",
       "      <td>0.860029</td>\n",
       "      <td>0.455822</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.833067</td>\n",
       "      <td>0.136010</td>\n",
       "      <td>1.132567</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>1.735459</td>\n",
       "      <td>0.335995</td>\n",
       "      <td>13.280481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.661809</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.241476</td>\n",
       "      <td>-0.174939</td>\n",
       "      <td>1.670931</td>\n",
       "      <td>-1.424675</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>1.019852</td>\n",
       "      <td>0.683656</td>\n",
       "      <td>0.478273</td>\n",
       "      <td>0.286818</td>\n",
       "      <td>0.141497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010682</td>\n",
       "      <td>0.059641</td>\n",
       "      <td>-0.371086</td>\n",
       "      <td>0.713261</td>\n",
       "      <td>-0.126430</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>22.678137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.717554</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.204780</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>0.642867</td>\n",
       "      <td>-1.025366</td>\n",
       "      <td>1.644496</td>\n",
       "      <td>-0.102401</td>\n",
       "      <td>1.113809</td>\n",
       "      <td>-1.072203</td>\n",
       "      <td>-0.491797</td>\n",
       "      <td>1.005130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.786360</td>\n",
       "      <td>0.429373</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>0.639385</td>\n",
       "      <td>1.897215</td>\n",
       "      <td>1.053077</td>\n",
       "      <td>2.790112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.029181</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   -0.246181 -1.274200 -0.950982 -0.705522  0.132961  0.814546  1.883581   \n",
       "1    1.948685  0.261073 -2.629703  0.005834 -0.352464  1.072183 -1.093752   \n",
       "2    0.615300  1.659502 -0.099698  0.817223  0.106429  0.114220  0.564847   \n",
       "3    0.887987 -0.017229  1.593608  0.885682  0.409091  1.043431  0.394262   \n",
       "4   -0.457293 -0.296993 -0.979472  0.810209 -1.433864 -1.018180 -0.199094   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.147637 -0.955530  0.393667 -0.281182  0.626307 -0.422972 -2.630214   \n",
       "996  1.310743  1.540612  0.637802  0.060178  0.820293  0.767454  0.222449   \n",
       "997 -0.552284  0.074332  0.477756  0.984183 -0.469270 -1.480282  1.335541   \n",
       "998 -0.241476 -0.174939  1.670931 -1.424675  0.311019  1.019852  0.683656   \n",
       "999  1.204780 -0.018272  0.642867 -1.025366  1.644496 -0.102401  1.113809   \n",
       "\n",
       "           X8        X9       X10  ...       X45       X46       X47  \\\n",
       "0   -0.188843  0.632661 -0.023047  ... -2.612764  0.869625  0.113842   \n",
       "1   -0.427244 -2.305874  0.670639  ...  1.201444  0.412904 -0.120547   \n",
       "2    0.073989 -0.303836  1.339558  ... -0.538605 -1.870271  1.196852   \n",
       "3   -1.698353  0.558425  0.933242  ...  1.074709 -0.562967 -0.608498   \n",
       "4   -0.996772 -0.402186  2.204502  ... -0.121076  0.549883  0.295039   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.163528  1.020511 -0.364770  ...  0.145645  1.335647 -0.485012   \n",
       "996 -0.112176 -0.966916  0.391996  ... -1.277102  0.842328  0.061312   \n",
       "997  1.114310  0.860029  0.455822  ... -0.833067  0.136010  1.132567   \n",
       "998  0.478273  0.286818  0.141497  ...  0.010682  0.059641 -0.371086   \n",
       "999 -1.072203 -0.491797  1.005130  ... -0.786360  0.429373  0.735366   \n",
       "\n",
       "          X48       X49       X50          y    d  true_cates  uuid  \n",
       "0    0.095730  0.716684 -0.302220  22.787988  1.0   19.738300     0  \n",
       "1   -1.460580  0.383900  0.445292   4.540934  0.0   11.616198     1  \n",
       "2   -2.239998  0.411400 -1.889548   2.802615  0.0   19.294353     2  \n",
       "3   -1.515747  0.491019 -0.125751  26.132472  1.0   24.358482     3  \n",
       "4   -0.636527 -0.490393 -1.062060  -3.520146  1.0   -6.477713     4  \n",
       "..        ...       ...       ...        ...  ...         ...   ...  \n",
       "995  0.668727 -1.590170  1.036452  30.132903  1.0   27.992916   995  \n",
       "996 -0.808946  0.333095  1.076270   2.968386  0.0   31.238688   996  \n",
       "997  0.304348  1.735459  0.335995  13.280481  1.0    9.661809   997  \n",
       "998  0.713261 -0.126430  0.164833  22.678137  1.0   22.717554   998  \n",
       "999  0.639385  1.897215  1.053077   2.790112  0.0   45.029181   999  \n",
       "\n",
       "[1000 rows x 54 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CamlCATE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Class Instantiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from caml import CamlCATE\n",
    "\n",
    "caml = CamlCATE(\n",
    "    df=df,\n",
    "    Y=\"y\",\n",
    "    T=\"d\",\n",
    "    X=[c for c in df.columns if \"X\" in c],\n",
    "    W=[c for c in df.columns if \"W\" in c],\n",
    "    uuid=\"uuid\",\n",
    "    discrete_treatment=True,\n",
    "    discrete_outcome=False,\n",
    "    seed=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nuissance Function AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[flaml.automl.logger: 08-24 19:16:52] {1680} INFO - task = regression\n",
      "[flaml.automl.logger: 08-24 19:16:52] {1691} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 08-24 19:16:52] {1789} INFO - Minimizing error metric: mse\n",
      "[flaml.automl.logger: 08-24 19:16:52] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 08-24 19:16:52] {2219} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:52] {2345} INFO - Estimated sufficient time budget=2923s. Estimated necessary time budget=21s.\n",
      "[flaml.automl.logger: 08-24 19:16:52] {2392} INFO -  at 0.3s,\testimator lgbm's best error=187.9247,\tbest estimator lgbm's best error=187.9247\n",
      "[flaml.automl.logger: 08-24 19:16:52] {2219} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:53] {2392} INFO -  at 0.8s,\testimator lgbm's best error=187.9247,\tbest estimator lgbm's best error=187.9247\n",
      "[flaml.automl.logger: 08-24 19:16:53] {2219} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:16:54] {2392} INFO -  at 1.4s,\testimator xgboost's best error=188.8995,\tbest estimator lgbm's best error=187.9247\n",
      "[flaml.automl.logger: 08-24 19:16:54] {2219} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:16:54] {2392} INFO -  at 2.2s,\testimator xgboost's best error=188.2206,\tbest estimator lgbm's best error=187.9247\n",
      "[flaml.automl.logger: 08-24 19:16:54] {2219} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 2.6s,\testimator lgbm's best error=167.8063,\tbest estimator lgbm's best error=167.8063\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 5, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 2.7s,\testimator extra_tree's best error=163.8129,\tbest estimator extra_tree's best error=163.8129\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 6, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 2.8s,\testimator extra_tree's best error=163.8129,\tbest estimator extra_tree's best error=163.8129\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 7, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 2.9s,\testimator extra_tree's best error=153.6308,\tbest estimator extra_tree's best error=153.6308\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 3.2s,\testimator lgbm's best error=167.8063,\tbest estimator extra_tree's best error=153.6308\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 9, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2392} INFO -  at 3.3s,\testimator extra_tree's best error=152.1775,\tbest estimator extra_tree's best error=152.1775\n",
      "[flaml.automl.logger: 08-24 19:16:55] {2219} INFO - iteration 10, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 3.5s,\testimator extra_tree's best error=150.5468,\tbest estimator extra_tree's best error=150.5468\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 11, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 3.6s,\testimator extra_tree's best error=150.5468,\tbest estimator extra_tree's best error=150.5468\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 12, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 3.7s,\testimator extra_tree's best error=150.5468,\tbest estimator extra_tree's best error=150.5468\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 13, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 3.8s,\testimator extra_tree's best error=148.3537,\tbest estimator extra_tree's best error=148.3537\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 14, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 4.0s,\testimator extra_tree's best error=148.3537,\tbest estimator extra_tree's best error=148.3537\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 4.1s,\testimator extra_tree's best error=148.3537,\tbest estimator extra_tree's best error=148.3537\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 16, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2392} INFO -  at 4.2s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:56] {2219} INFO - iteration 17, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2392} INFO -  at 4.6s,\testimator lgbm's best error=150.7367,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2219} INFO - iteration 18, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2392} INFO -  at 4.8s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2219} INFO - iteration 19, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2392} INFO -  at 4.9s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2219} INFO - iteration 20, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2392} INFO -  at 5.1s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:57] {2219} INFO - iteration 21, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:58] {2392} INFO -  at 5.4s,\testimator lgbm's best error=150.7367,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:58] {2219} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:16:58] {2392} INFO -  at 6.4s,\testimator lgbm's best error=150.7367,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:58] {2219} INFO - iteration 23, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 6.5s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 24, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 6.6s,\testimator rf's best error=152.4208,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 6.8s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 26, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 6.9s,\testimator rf's best error=151.6553,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 7.1s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 28, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 7.2s,\testimator extra_tree's best error=147.4144,\tbest estimator extra_tree's best error=147.4144\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 29, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2392} INFO -  at 7.4s,\testimator extra_tree's best error=144.9010,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:16:59] {2219} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:00] {2392} INFO -  at 8.0s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:00] {2219} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2392} INFO -  at 8.4s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2219} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2392} INFO -  at 8.8s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2219} INFO - iteration 33, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2392} INFO -  at 8.9s,\testimator extra_tree's best error=144.9010,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:01] {2219} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2392} INFO -  at 10.5s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2219} INFO - iteration 35, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2392} INFO -  at 10.6s,\testimator rf's best error=151.6553,\tbest estimator extra_tree's best error=144.9010\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2219} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2392} INFO -  at 10.8s,\testimator extra_tree's best error=144.3115,\tbest estimator extra_tree's best error=144.3115\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2219} INFO - iteration 37, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2392} INFO -  at 11.0s,\testimator extra_tree's best error=144.3115,\tbest estimator extra_tree's best error=144.3115\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2219} INFO - iteration 38, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2392} INFO -  at 11.4s,\testimator extra_tree's best error=141.9738,\tbest estimator extra_tree's best error=141.9738\n",
      "[flaml.automl.logger: 08-24 19:17:03] {2219} INFO - iteration 39, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2392} INFO -  at 11.6s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2219} INFO - iteration 40, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2392} INFO -  at 12.0s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2219} INFO - iteration 41, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2392} INFO -  at 12.4s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:04] {2219} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2392} INFO -  at 12.5s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2219} INFO - iteration 43, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2392} INFO -  at 12.7s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2219} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2392} INFO -  at 12.9s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2219} INFO - iteration 45, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2392} INFO -  at 13.3s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:05] {2219} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:06] {2392} INFO -  at 13.8s,\testimator xgboost's best error=188.2206,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:06] {2219} INFO - iteration 47, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:06] {2392} INFO -  at 14.1s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:06] {2219} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:07] {2392} INFO -  at 14.9s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:07] {2219} INFO - iteration 49, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:07] {2392} INFO -  at 15.2s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:07] {2219} INFO - iteration 50, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2392} INFO -  at 15.5s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2219} INFO - iteration 51, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2392} INFO -  at 15.6s,\testimator rf's best error=151.6553,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2219} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2392} INFO -  at 15.8s,\testimator rf's best error=150.3651,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:08] {2219} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2392} INFO -  at 16.6s,\testimator lgbm's best error=145.5884,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2219} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2392} INFO -  at 17.1s,\testimator lgbm's best error=144.6837,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2219} INFO - iteration 55, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2392} INFO -  at 17.4s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:09] {2219} INFO - iteration 56, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2392} INFO -  at 17.6s,\testimator rf's best error=150.3651,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2219} INFO - iteration 57, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2392} INFO -  at 17.8s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2219} INFO - iteration 58, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2392} INFO -  at 18.1s,\testimator extra_tree's best error=140.7316,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2219} INFO - iteration 59, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2392} INFO -  at 18.2s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.7316\n",
      "[flaml.automl.logger: 08-24 19:17:10] {2219} INFO - iteration 60, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2392} INFO -  at 18.5s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2219} INFO - iteration 61, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2392} INFO -  at 18.6s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2219} INFO - iteration 62, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2392} INFO -  at 18.8s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2219} INFO - iteration 63, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2392} INFO -  at 19.1s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2219} INFO - iteration 64, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2392} INFO -  at 19.3s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:11] {2219} INFO - iteration 65, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2392} INFO -  at 19.4s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2219} INFO - iteration 66, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2392} INFO -  at 19.6s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2219} INFO - iteration 67, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2392} INFO -  at 19.9s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2219} INFO - iteration 68, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2392} INFO -  at 20.0s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2219} INFO - iteration 69, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2392} INFO -  at 20.2s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:12] {2219} INFO - iteration 70, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2392} INFO -  at 20.5s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2219} INFO - iteration 71, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2392} INFO -  at 20.8s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2219} INFO - iteration 72, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2392} INFO -  at 21.0s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2219} INFO - iteration 73, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2392} INFO -  at 21.2s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:13] {2219} INFO - iteration 74, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 21.5s,\testimator extra_tree's best error=140.4055,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 75, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 21.6s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 76, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 21.7s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.4055\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 77, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 21.9s,\testimator extra_tree's best error=140.1306,\tbest estimator extra_tree's best error=140.1306\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 78, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 22.1s,\testimator extra_tree's best error=140.1306,\tbest estimator extra_tree's best error=140.1306\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 79, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2392} INFO -  at 22.3s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=140.1306\n",
      "[flaml.automl.logger: 08-24 19:17:14] {2219} INFO - iteration 80, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2392} INFO -  at 22.5s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2219} INFO - iteration 81, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2392} INFO -  at 22.7s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2219} INFO - iteration 82, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2392} INFO -  at 22.9s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2219} INFO - iteration 83, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2392} INFO -  at 23.1s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2219} INFO - iteration 84, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2392} INFO -  at 23.3s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:15] {2219} INFO - iteration 85, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2392} INFO -  at 23.6s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2219} INFO - iteration 86, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2392} INFO -  at 23.8s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2219} INFO - iteration 87, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2392} INFO -  at 24.0s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2219} INFO - iteration 88, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2392} INFO -  at 24.2s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:16] {2219} INFO - iteration 89, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 24.4s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 90, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 24.6s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 91, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 24.8s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 92, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 25.0s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 93, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 25.2s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 94, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2392} INFO -  at 25.3s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:17] {2219} INFO - iteration 95, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2392} INFO -  at 25.5s,\testimator extra_tree's best error=138.7896,\tbest estimator extra_tree's best error=138.7896\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2219} INFO - iteration 96, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2392} INFO -  at 25.7s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2219} INFO - iteration 97, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2392} INFO -  at 25.9s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2219} INFO - iteration 98, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2392} INFO -  at 26.1s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2219} INFO - iteration 99, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2392} INFO -  at 26.3s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:18] {2219} INFO - iteration 100, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2392} INFO -  at 26.6s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2219} INFO - iteration 101, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2392} INFO -  at 26.8s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2219} INFO - iteration 102, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2392} INFO -  at 26.8s,\testimator lgbm's best error=144.6837,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2219} INFO - iteration 103, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2392} INFO -  at 27.0s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2219} INFO - iteration 104, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2392} INFO -  at 27.2s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:19] {2219} INFO - iteration 105, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 27.4s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 106, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 27.6s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 107, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 27.8s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 108, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 28.0s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 109, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 28.2s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 110, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2392} INFO -  at 28.3s,\testimator lgbm's best error=144.6837,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:20] {2219} INFO - iteration 111, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2392} INFO -  at 28.4s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2219} INFO - iteration 112, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2392} INFO -  at 28.7s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2219} INFO - iteration 113, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2392} INFO -  at 28.9s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2219} INFO - iteration 114, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2392} INFO -  at 29.1s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2219} INFO - iteration 115, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2392} INFO -  at 29.2s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:21] {2219} INFO - iteration 116, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 29.4s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 117, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 29.6s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 118, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 29.7s,\testimator rf's best error=148.9183,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 119, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 29.9s,\testimator extra_tree's best error=138.7587,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 120, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 30.0s,\testimator xgb_limitdepth's best error=180.1191,\tbest estimator extra_tree's best error=138.7587\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2628} INFO - retrain extra_tree for 0.1s\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2631} INFO - retrained model: ExtraTreesRegressor(max_features=0.9633250950818248, max_leaf_nodes=181,\n",
      "                    n_estimators=74, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1931} INFO - fit succeeded\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1932} INFO - Time taken to find the best model: 25.719156980514526\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1680} INFO - task = regression\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1691} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1789} INFO - Minimizing error metric: mse\n",
      "[flaml.automl.logger: 08-24 19:17:22] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2345} INFO - Estimated sufficient time budget=196s. Estimated necessary time budget=1s.\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.0s,\testimator lgbm's best error=118.3923,\tbest estimator lgbm's best error=118.3923\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.0s,\testimator lgbm's best error=118.3923,\tbest estimator lgbm's best error=118.3923\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 2, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.1s,\testimator lgbm's best error=64.8112,\tbest estimator lgbm's best error=64.8112\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.1s,\testimator xgboost's best error=118.7740,\tbest estimator lgbm's best error=64.8112\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.2s,\testimator lgbm's best error=64.8112,\tbest estimator lgbm's best error=64.8112\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 5, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.2s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 6, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2392} INFO -  at 0.3s,\testimator extra_tree's best error=31.0266,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:22] {2219} INFO - iteration 7, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.4s,\testimator rf's best error=25.6665,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.4s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.4s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.5s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 11, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.6s,\testimator rf's best error=25.6665,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 12, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.7s,\testimator rf's best error=16.5938,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 13, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.7s,\testimator xgboost's best error=114.9532,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.8s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 15, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.9s,\testimator extra_tree's best error=31.0266,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 0.9s,\testimator lgbm's best error=9.8565,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 1.0s,\testimator extra_tree's best error=17.6576,\tbest estimator lgbm's best error=9.8565\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 1.1s,\testimator lgbm's best error=4.6623,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 19, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2392} INFO -  at 1.2s,\testimator rf's best error=6.2649,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:23] {2219} INFO - iteration 20, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.3s,\testimator rf's best error=4.6985,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 21, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.5s,\testimator rf's best error=4.6985,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.6s,\testimator lgbm's best error=4.6623,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.6s,\testimator lgbm's best error=4.6623,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.6s,\testimator xgboost's best error=112.3495,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.8s,\testimator extra_tree's best error=6.7415,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 26, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 1.9s,\testimator rf's best error=4.6985,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 27, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 2.0s,\testimator extra_tree's best error=6.3970,\tbest estimator lgbm's best error=4.6623\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 28, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 2.2s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2392} INFO -  at 2.3s,\testimator xgboost's best error=112.3495,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:24] {2219} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.3s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 33, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.6s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.6s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 35, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.6s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.8s,\testimator extra_tree's best error=6.3970,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 37, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.8s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 38, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 2.9s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 39, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 3.0s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 40, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2392} INFO -  at 3.2s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:25] {2219} INFO - iteration 41, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.3s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.3s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 43, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.5s,\testimator rf's best error=4.5143,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.6s,\testimator extra_tree's best error=6.3970,\tbest estimator rf's best error=4.5143\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 45, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.8s,\testimator rf's best error=2.6292,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 46, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 3.8s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 47, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 4.0s,\testimator rf's best error=2.6292,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 4.0s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 49, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 4.1s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 50, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2392} INFO -  at 4.3s,\testimator rf's best error=2.6292,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:26] {2219} INFO - iteration 51, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.3s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 52, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.6292\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 55, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.6s,\testimator rf's best error=2.5131,\tbest estimator rf's best error=2.5131\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 56, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.6s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.5131\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.6s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.5131\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 58, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.7s,\testimator rf's best error=2.5131,\tbest estimator rf's best error=2.5131\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 59, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 4.8s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.5131\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 60, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 5.0s,\testimator rf's best error=2.2217,\tbest estimator rf's best error=2.2217\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 61, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 5.0s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.2217\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 62, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2392} INFO -  at 5.1s,\testimator rf's best error=2.2217,\tbest estimator rf's best error=2.2217\n",
      "[flaml.automl.logger: 08-24 19:17:27] {2219} INFO - iteration 63, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2392} INFO -  at 5.4s,\testimator rf's best error=2.0460,\tbest estimator rf's best error=2.0460\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2219} INFO - iteration 64, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2392} INFO -  at 5.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.0460\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2219} INFO - iteration 65, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2392} INFO -  at 5.6s,\testimator rf's best error=2.0460,\tbest estimator rf's best error=2.0460\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2219} INFO - iteration 66, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2392} INFO -  at 6.0s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:28] {2219} INFO - iteration 67, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2392} INFO -  at 6.4s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2219} INFO - iteration 68, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2392} INFO -  at 6.4s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2219} INFO - iteration 69, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2392} INFO -  at 7.0s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:29] {2219} INFO - iteration 70, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.6s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 71, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.6s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.7s,\testimator xgboost's best error=87.8633,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.7s,\testimator xgboost's best error=11.1233,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.7s,\testimator xgboost's best error=11.1233,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 75, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.7s,\testimator xgboost's best error=11.1233,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 76, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.8s,\testimator xgboost's best error=4.4563,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 77, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.8s,\testimator xgboost's best error=4.4563,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 78, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 7.9s,\testimator xgboost's best error=2.8555,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 79, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 8.0s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 80, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 8.0s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 81, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2392} INFO -  at 8.1s,\testimator lgbm's best error=4.6623,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:30] {2219} INFO - iteration 82, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2392} INFO -  at 8.4s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2219} INFO - iteration 83, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2392} INFO -  at 8.7s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2219} INFO - iteration 84, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2392} INFO -  at 8.8s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2219} INFO - iteration 85, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2392} INFO -  at 9.2s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:31] {2219} INFO - iteration 86, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 9.6s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 87, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 9.6s,\testimator lgbm's best error=3.2432,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 88, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 9.7s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 89, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 9.8s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 90, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 9.9s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 91, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 10.0s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 92, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 10.0s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 93, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 10.1s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 94, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 10.2s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 95, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2392} INFO -  at 10.2s,\testimator lgbm's best error=3.2432,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:32] {2219} INFO - iteration 96, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.3s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 97, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.4s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 98, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.5s,\testimator xgb_limitdepth's best error=2.3352,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 99, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.7s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 100, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.7s,\testimator lgbm's best error=3.2432,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 101, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.8s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 102, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 10.9s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 103, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 11.0s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 104, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 11.2s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 105, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2392} INFO -  at 11.3s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:33] {2219} INFO - iteration 106, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.4s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 107, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.5s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 108, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.5s,\testimator xgboost's best error=2.7917,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 109, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.6s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 110, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.7s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 111, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.7s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 112, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.8s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 113, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 11.9s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 114, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2392} INFO -  at 12.0s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:34] {2219} INFO - iteration 115, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2392} INFO -  at 12.5s,\testimator rf's best error=2.0441,\tbest estimator rf's best error=2.0441\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2219} INFO - iteration 116, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2392} INFO -  at 12.9s,\testimator rf's best error=2.0217,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2219} INFO - iteration 117, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2392} INFO -  at 13.0s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:35] {2219} INFO - iteration 118, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.3s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 119, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.4s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 120, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.5s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 121, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.7s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 122, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.8s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 123, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 13.9s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 124, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 14.0s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 125, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 14.0s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 126, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 14.1s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 127, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2392} INFO -  at 14.1s,\testimator lgbm's best error=3.1480,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:36] {2219} INFO - iteration 128, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 14.5s,\testimator rf's best error=2.0217,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 129, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 14.6s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 130, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 14.7s,\testimator extra_tree's best error=4.1362,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 131, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 14.8s,\testimator extra_tree's best error=4.1362,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 132, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 14.9s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 133, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2392} INFO -  at 15.2s,\testimator rf's best error=2.0217,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:37] {2219} INFO - iteration 134, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2392} INFO -  at 15.3s,\testimator extra_tree's best error=4.1362,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2219} INFO - iteration 135, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2392} INFO -  at 15.5s,\testimator extra_tree's best error=3.0776,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2392} INFO -  at 15.6s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2219} INFO - iteration 137, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2392} INFO -  at 15.7s,\testimator extra_tree's best error=2.5435,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2219} INFO - iteration 138, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2392} INFO -  at 16.3s,\testimator rf's best error=2.0217,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:38] {2219} INFO - iteration 139, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 16.4s,\testimator extra_tree's best error=2.5435,\tbest estimator rf's best error=2.0217\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 140, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 16.6s,\testimator extra_tree's best error=1.8201,\tbest estimator extra_tree's best error=1.8201\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 141, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 16.7s,\testimator extra_tree's best error=1.8201,\tbest estimator extra_tree's best error=1.8201\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 142, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 16.9s,\testimator extra_tree's best error=1.8201,\tbest estimator extra_tree's best error=1.8201\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 143, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 17.1s,\testimator extra_tree's best error=1.7962,\tbest estimator extra_tree's best error=1.7962\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 144, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2392} INFO -  at 17.2s,\testimator extra_tree's best error=1.7962,\tbest estimator extra_tree's best error=1.7962\n",
      "[flaml.automl.logger: 08-24 19:17:39] {2219} INFO - iteration 145, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.4s,\testimator extra_tree's best error=1.7962,\tbest estimator extra_tree's best error=1.7962\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 146, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.5s,\testimator extra_tree's best error=1.7962,\tbest estimator extra_tree's best error=1.7962\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 147, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.6s,\testimator xgboost's best error=2.7917,\tbest estimator extra_tree's best error=1.7962\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 148, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.8s,\testimator extra_tree's best error=1.6636,\tbest estimator extra_tree's best error=1.6636\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 149, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.8s,\testimator xgboost's best error=2.7917,\tbest estimator extra_tree's best error=1.6636\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 150, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 17.9s,\testimator extra_tree's best error=1.6636,\tbest estimator extra_tree's best error=1.6636\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 151, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2392} INFO -  at 18.2s,\testimator extra_tree's best error=1.6343,\tbest estimator extra_tree's best error=1.6343\n",
      "[flaml.automl.logger: 08-24 19:17:40] {2219} INFO - iteration 152, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 18.4s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 153, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 18.6s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 154, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 18.7s,\testimator lgbm's best error=3.1480,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 155, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 18.9s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 156, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 19.1s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 157, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2392} INFO -  at 19.2s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:41] {2219} INFO - iteration 158, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2392} INFO -  at 19.4s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2219} INFO - iteration 159, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2392} INFO -  at 19.7s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2219} INFO - iteration 160, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2392} INFO -  at 19.9s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2219} INFO - iteration 161, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2392} INFO -  at 20.0s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2219} INFO - iteration 162, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2392} INFO -  at 20.2s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:42] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2392} INFO -  at 20.5s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2219} INFO - iteration 164, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2392} INFO -  at 20.6s,\testimator extra_tree's best error=1.5749,\tbest estimator extra_tree's best error=1.5749\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2219} INFO - iteration 165, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2392} INFO -  at 20.8s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2219} INFO - iteration 166, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2392} INFO -  at 21.0s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2219} INFO - iteration 167, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2392} INFO -  at 21.2s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:43] {2219} INFO - iteration 168, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 21.5s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 169, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 21.6s,\testimator xgboost's best error=2.7917,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 170, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 21.6s,\testimator lgbm's best error=3.1480,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 171, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 21.7s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 172, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 21.9s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 173, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2392} INFO -  at 22.1s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:44] {2219} INFO - iteration 174, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 22.3s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 175, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 22.4s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 22.5s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 177, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 22.7s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 178, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 22.7s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 179, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 23.0s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 180, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2392} INFO -  at 23.2s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:45] {2219} INFO - iteration 181, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.4s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 182, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.4s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 183, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.5s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 184, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.6s,\testimator xgboost's best error=2.7917,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 185, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.8s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 186, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 23.8s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 187, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 24.1s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 188, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2392} INFO -  at 24.2s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:46] {2219} INFO - iteration 189, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 24.3s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 190, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 24.5s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 191, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 24.6s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 192, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 24.8s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 25.0s,\testimator extra_tree's best error=1.5727,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 194, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 25.1s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 195, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 25.1s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 196, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2392} INFO -  at 25.2s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5727\n",
      "[flaml.automl.logger: 08-24 19:17:47] {2219} INFO - iteration 197, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2392} INFO -  at 25.5s,\testimator extra_tree's best error=1.5726,\tbest estimator extra_tree's best error=1.5726\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2219} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2392} INFO -  at 25.9s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2219} INFO - iteration 199, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2392} INFO -  at 26.2s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:48] {2219} INFO - iteration 200, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2392} INFO -  at 26.5s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2219} INFO - iteration 201, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2392} INFO -  at 26.6s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2219} INFO - iteration 202, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2392} INFO -  at 26.8s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2219} INFO - iteration 203, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2392} INFO -  at 27.1s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:49] {2219} INFO - iteration 204, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2392} INFO -  at 27.4s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2219} INFO - iteration 205, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2392} INFO -  at 27.5s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2219} INFO - iteration 206, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2392} INFO -  at 27.7s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2219} INFO - iteration 207, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2392} INFO -  at 28.0s,\testimator extra_tree's best error=1.5673,\tbest estimator extra_tree's best error=1.5673\n",
      "[flaml.automl.logger: 08-24 19:17:50] {2219} INFO - iteration 208, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2392} INFO -  at 28.5s,\testimator extra_tree's best error=1.5585,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2219} INFO - iteration 209, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2392} INFO -  at 28.8s,\testimator extra_tree's best error=1.5585,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2219} INFO - iteration 210, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2392} INFO -  at 28.9s,\testimator extra_tree's best error=1.5585,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:51] {2219} INFO - iteration 211, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 29.7s,\testimator extra_tree's best error=1.5585,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 212, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 29.7s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 213, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 29.8s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 214, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 29.9s,\testimator xgb_limitdepth's best error=2.1344,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 215, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 29.9s,\testimator lgbm's best error=2.6836,\tbest estimator extra_tree's best error=1.5585\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2628} INFO - retrain extra_tree for 0.2s\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2631} INFO - retrained model: ExtraTreesRegressor(max_leaf_nodes=68, n_estimators=277, n_jobs=-1,\n",
      "                    random_state=12032022)\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1931} INFO - fit succeeded\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1932} INFO - Time taken to find the best model: 28.48213768005371\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1680} INFO - task = classification\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1691} INFO - Evaluation method: cv\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1789} INFO - Minimizing error metric: log_loss\n",
      "[flaml.automl.logger: 08-24 19:17:52] {1901} INFO - List of ML learners in AutoML Run: ['lgbm', 'rf', 'xgboost', 'extra_tree', 'xgb_limitdepth']\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 0, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2345} INFO - Estimated sufficient time budget=281s. Estimated necessary time budget=2s.\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 0.0s,\testimator lgbm's best error=0.6711,\tbest estimator lgbm's best error=0.6711\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 1, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 0.1s,\testimator lgbm's best error=0.6711,\tbest estimator lgbm's best error=0.6711\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 2, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2392} INFO -  at 0.2s,\testimator xgboost's best error=0.6709,\tbest estimator xgboost's best error=0.6709\n",
      "[flaml.automl.logger: 08-24 19:17:52] {2219} INFO - iteration 3, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.2s,\testimator xgboost's best error=0.6709,\tbest estimator xgboost's best error=0.6709\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 4, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.3s,\testimator lgbm's best error=0.6637,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 5, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.4s,\testimator extra_tree's best error=0.6948,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 6, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.5s,\testimator rf's best error=0.6859,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 7, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.6s,\testimator rf's best error=0.6721,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 8, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.6s,\testimator lgbm's best error=0.6637,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 9, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.6s,\testimator lgbm's best error=0.6637,\tbest estimator lgbm's best error=0.6637\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 10, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.6596,\tbest estimator lgbm's best error=0.6596\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 11, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.7s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 12, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.8s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 13, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.9s,\testimator rf's best error=0.6721,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 14, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.9s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 15, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 0.9s,\testimator xgboost's best error=0.6675,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 16, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 1.0s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 17, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 1.1s,\testimator extra_tree's best error=0.6854,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 18, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2392} INFO -  at 1.2s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:53] {2219} INFO - iteration 19, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.3s,\testimator rf's best error=0.6721,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 20, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.4s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 21, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.5s,\testimator rf's best error=0.6721,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 22, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.5s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 23, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.6s,\testimator lgbm's best error=0.6590,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 24, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.6s,\testimator xgboost's best error=0.6675,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 25, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.7s,\testimator extra_tree's best error=0.6854,\tbest estimator lgbm's best error=0.6590\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 26, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.7s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 27, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.9s,\testimator rf's best error=0.6721,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 28, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 1.9s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 29, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 2.0s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 30, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 2.0s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 31, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 2.1s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 32, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 2.1s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 33, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2392} INFO -  at 2.1s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:54] {2219} INFO - iteration 34, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.2s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 35, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.3s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 36, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.4s,\testimator extra_tree's best error=0.6854,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 37, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.5s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 38, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.5s,\testimator xgboost's best error=0.6594,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 39, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.5s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 40, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.6s,\testimator xgboost's best error=0.6565,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 41, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.6s,\testimator xgboost's best error=0.6565,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 42, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.8s,\testimator lgbm's best error=0.6545,\tbest estimator lgbm's best error=0.6545\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 43, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.8s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 44, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 2.9s,\testimator extra_tree's best error=0.6848,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 45, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 3.1s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 46, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 3.1s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 47, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2392} INFO -  at 3.2s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:55] {2219} INFO - iteration 48, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.2s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 49, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.3s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 50, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.3s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 51, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.4s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 52, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.5s,\testimator rf's best error=0.6714,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 53, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.6s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 54, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.6s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 55, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.6s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 56, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.7s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 57, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.8s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 58, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.9s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 59, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 3.9s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 60, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 4.1s,\testimator xgboost's best error=0.6523,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 61, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2392} INFO -  at 4.1s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6523\n",
      "[flaml.automl.logger: 08-24 19:17:56] {2219} INFO - iteration 62, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 4.4s,\testimator xgboost's best error=0.6522,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 63, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 4.4s,\testimator xgboost's best error=0.6522,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 64, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 4.6s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 65, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 4.7s,\testimator rf's best error=0.6714,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 66, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 4.9s,\testimator xgboost's best error=0.6522,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 67, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 5.0s,\testimator rf's best error=0.6714,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 68, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 5.0s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 69, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2392} INFO -  at 5.1s,\testimator xgboost's best error=0.6522,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:57] {2219} INFO - iteration 70, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 5.2s,\testimator xgboost's best error=0.6522,\tbest estimator xgboost's best error=0.6522\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 71, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 5.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 72, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 5.5s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 73, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 5.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 74, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 6.1s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 75, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2392} INFO -  at 6.2s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:58] {2219} INFO - iteration 76, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.2s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 77, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.3s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 78, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 79, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.5s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 80, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 81, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 6.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 82, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2392} INFO -  at 7.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:17:59] {2219} INFO - iteration 83, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 7.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 84, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 7.7s,\testimator xgb_limitdepth's best error=0.7075,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 85, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 7.8s,\testimator xgb_limitdepth's best error=0.7064,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 86, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 7.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 87, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 8.1s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 88, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 8.1s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 89, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2392} INFO -  at 8.1s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:00] {2219} INFO - iteration 90, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2392} INFO -  at 8.2s,\testimator xgb_limitdepth's best error=0.7064,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2219} INFO - iteration 91, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2392} INFO -  at 8.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2219} INFO - iteration 92, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2392} INFO -  at 8.5s,\testimator xgb_limitdepth's best error=0.6973,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2219} INFO - iteration 93, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2392} INFO -  at 8.7s,\testimator xgb_limitdepth's best error=0.6973,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2219} INFO - iteration 94, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2392} INFO -  at 8.8s,\testimator extra_tree's best error=0.6848,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:01] {2219} INFO - iteration 95, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.3s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 96, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.5s,\testimator xgb_limitdepth's best error=0.6973,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 97, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 98, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 99, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.8s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 100, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 9.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 101, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 10.1s,\testimator xgb_limitdepth's best error=0.6638,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 102, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2392} INFO -  at 10.1s,\testimator xgb_limitdepth's best error=0.6638,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:02] {2219} INFO - iteration 103, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 10.3s,\testimator rf's best error=0.6714,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 104, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 10.4s,\testimator xgb_limitdepth's best error=0.6638,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 105, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 10.5s,\testimator xgb_limitdepth's best error=0.6638,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 106, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 10.7s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 107, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 10.8s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 108, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2392} INFO -  at 11.0s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:03] {2219} INFO - iteration 109, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2392} INFO -  at 11.2s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2219} INFO - iteration 110, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2392} INFO -  at 11.5s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2219} INFO - iteration 111, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2392} INFO -  at 11.6s,\testimator rf's best error=0.6694,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2219} INFO - iteration 112, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2392} INFO -  at 11.8s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2219} INFO - iteration 113, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2392} INFO -  at 12.1s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:04] {2219} INFO - iteration 114, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.3s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 115, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 116, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.5s,\testimator rf's best error=0.6694,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 117, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.6s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 118, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.7s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 119, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 12.9s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 120, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2392} INFO -  at 13.2s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:05] {2219} INFO - iteration 121, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 13.2s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 122, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 13.4s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 123, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 13.5s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 124, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 13.7s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 125, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 13.9s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 126, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2392} INFO -  at 14.1s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:06] {2219} INFO - iteration 127, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2392} INFO -  at 14.3s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2219} INFO - iteration 128, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2392} INFO -  at 14.4s,\testimator rf's best error=0.6622,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2219} INFO - iteration 129, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2392} INFO -  at 14.8s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2219} INFO - iteration 130, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2392} INFO -  at 15.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2219} INFO - iteration 131, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2392} INFO -  at 15.1s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:07] {2219} INFO - iteration 132, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2392} INFO -  at 15.2s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2219} INFO - iteration 133, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2392} INFO -  at 15.6s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2219} INFO - iteration 134, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2392} INFO -  at 15.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2219} INFO - iteration 135, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2392} INFO -  at 15.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2219} INFO - iteration 136, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2392} INFO -  at 16.2s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:08] {2219} INFO - iteration 137, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 16.2s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 138, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 16.4s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 139, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 16.6s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 140, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 16.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 141, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 16.8s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 142, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 17.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 143, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2392} INFO -  at 17.1s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:09] {2219} INFO - iteration 144, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2392} INFO -  at 17.3s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2219} INFO - iteration 145, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2392} INFO -  at 17.3s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2219} INFO - iteration 146, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2392} INFO -  at 17.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2219} INFO - iteration 147, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2392} INFO -  at 17.9s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2219} INFO - iteration 148, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2392} INFO -  at 18.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:10] {2219} INFO - iteration 149, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 18.3s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 150, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 18.5s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 151, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 18.5s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 152, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 18.6s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 153, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 18.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 154, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2392} INFO -  at 19.0s,\testimator lgbm's best error=0.6545,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:11] {2219} INFO - iteration 155, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 19.3s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 156, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 19.7s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 157, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 19.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 158, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 19.9s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 159, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 20.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 160, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 20.0s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 161, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2392} INFO -  at 20.1s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:12] {2219} INFO - iteration 162, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 20.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 163, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 20.5s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 164, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 20.7s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 165, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 20.8s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 166, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 21.0s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 167, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2392} INFO -  at 21.1s,\testimator extra_tree's best error=0.6831,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:13] {2219} INFO - iteration 168, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 21.3s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 169, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 21.4s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 170, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 21.6s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 171, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 21.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 172, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 21.9s,\testimator extra_tree's best error=0.6831,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 173, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 22.0s,\testimator extra_tree's best error=0.6742,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 174, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2392} INFO -  at 22.1s,\testimator extra_tree's best error=0.6742,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:14] {2219} INFO - iteration 175, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.2s,\testimator extra_tree's best error=0.6742,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 176, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.3s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 177, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.5s,\testimator extra_tree's best error=0.6711,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 178, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.6s,\testimator extra_tree's best error=0.6711,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 179, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.7s,\testimator extra_tree's best error=0.6711,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 180, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.8s,\testimator extra_tree's best error=0.6711,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 181, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 22.9s,\testimator extra_tree's best error=0.6665,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 182, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2392} INFO -  at 23.2s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:15] {2219} INFO - iteration 183, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.2s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 184, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.4s,\testimator extra_tree's best error=0.6665,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 185, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.5s,\testimator extra_tree's best error=0.6665,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 186, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 187, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.7s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 188, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 189, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 23.9s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 190, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 24.0s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 191, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2392} INFO -  at 24.1s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:16] {2219} INFO - iteration 192, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.2s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 193, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.3s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 194, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.7s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 195, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.7s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 196, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.8s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 197, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 24.9s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 198, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 25.0s,\testimator extra_tree's best error=0.6618,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 199, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2392} INFO -  at 25.1s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:17] {2219} INFO - iteration 200, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 25.3s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 201, current learner rf\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 25.5s,\testimator rf's best error=0.6616,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 202, current learner xgboost\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 25.6s,\testimator xgboost's best error=0.6508,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 203, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 25.7s,\testimator extra_tree's best error=0.6529,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 204, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 25.9s,\testimator extra_tree's best error=0.6529,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 205, current learner xgb_limitdepth\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 26.0s,\testimator xgb_limitdepth's best error=0.6582,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 206, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2392} INFO -  at 26.1s,\testimator extra_tree's best error=0.6529,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:18] {2219} INFO - iteration 207, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2392} INFO -  at 26.3s,\testimator extra_tree's best error=0.6525,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2219} INFO - iteration 208, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2392} INFO -  at 26.5s,\testimator extra_tree's best error=0.6516,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2219} INFO - iteration 209, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2392} INFO -  at 26.6s,\testimator extra_tree's best error=0.6516,\tbest estimator xgboost's best error=0.6508\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2219} INFO - iteration 210, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2392} INFO -  at 26.8s,\testimator extra_tree's best error=0.6504,\tbest estimator extra_tree's best error=0.6504\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2219} INFO - iteration 211, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2392} INFO -  at 27.1s,\testimator extra_tree's best error=0.6504,\tbest estimator extra_tree's best error=0.6504\n",
      "[flaml.automl.logger: 08-24 19:18:19] {2219} INFO - iteration 212, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 27.2s,\testimator extra_tree's best error=0.6504,\tbest estimator extra_tree's best error=0.6504\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 213, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 27.4s,\testimator extra_tree's best error=0.6499,\tbest estimator extra_tree's best error=0.6499\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 214, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 27.6s,\testimator extra_tree's best error=0.6499,\tbest estimator extra_tree's best error=0.6499\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 215, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 27.7s,\testimator extra_tree's best error=0.6499,\tbest estimator extra_tree's best error=0.6499\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 216, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 27.9s,\testimator extra_tree's best error=0.6498,\tbest estimator extra_tree's best error=0.6498\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 217, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 28.1s,\testimator extra_tree's best error=0.6498,\tbest estimator extra_tree's best error=0.6498\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 218, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2392} INFO -  at 28.2s,\testimator extra_tree's best error=0.6498,\tbest estimator extra_tree's best error=0.6498\n",
      "[flaml.automl.logger: 08-24 19:18:20] {2219} INFO - iteration 219, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 28.4s,\testimator extra_tree's best error=0.6498,\tbest estimator extra_tree's best error=0.6498\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 220, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 28.5s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 221, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 28.6s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 222, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 28.8s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 223, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 28.9s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 224, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 29.0s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 225, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2392} INFO -  at 29.2s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:21] {2219} INFO - iteration 226, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.3s,\testimator extra_tree's best error=0.6491,\tbest estimator extra_tree's best error=0.6491\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 227, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.4s,\testimator extra_tree's best error=0.6482,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 228, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.6s,\testimator extra_tree's best error=0.6482,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 229, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.7s,\testimator extra_tree's best error=0.6482,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 230, current learner extra_tree\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.9s,\testimator extra_tree's best error=0.6482,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 231, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.9s,\testimator lgbm's best error=0.6545,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 232, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 29.9s,\testimator lgbm's best error=0.6545,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 233, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 30.0s,\testimator lgbm's best error=0.6545,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2219} INFO - iteration 234, current learner lgbm\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2392} INFO -  at 30.0s,\testimator lgbm's best error=0.6545,\tbest estimator extra_tree's best error=0.6482\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2628} INFO - retrain extra_tree for 0.1s\n",
      "[flaml.automl.logger: 08-24 19:18:22] {2631} INFO - retrained model: ExtraTreesClassifier(criterion='entropy', max_features=1.0, max_leaf_nodes=11,\n",
      "                     n_estimators=18, n_jobs=-1, random_state=12032022)\n",
      "[flaml.automl.logger: 08-24 19:18:22] {1931} INFO - fit succeeded\n",
      "[flaml.automl.logger: 08-24 19:18:22] {1932} INFO - Time taken to find the best model: 29.43795657157898\n"
     ]
    }
   ],
   "source": [
    "caml.auto_nuisance_functions(\n",
    "    flaml_Y_kwargs={\"time_budget\": 30},\n",
    "    flaml_T_kwargs={\"time_budget\": 30},\n",
    "    use_ray=False,\n",
    "    use_spark=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit and ensemble CATE models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-24 19:18:23,636\tINFO worker.py:1772 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32m127.0.0.1:8265 \u001b[39m\u001b[22m\n",
      "\u001b[36m(fit_model pid=544311)\u001b[0m The final model has a nonzero intercept for at least one outcome; it will be subtracted, but consider fitting a model without an intercept if possible.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08/24/24 19:20:22] </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> Ensemble Estimator RScore: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.87284513990122</span>                                <a href=\"file:///home/jakep/projects/caml/caml/core/cate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/cate.py#849\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">849</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08/24/24 19:20:22]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Ensemble Estimator RScore: \u001b[1;36m0.87284513990122\u001b[0m                                \u001b]8;id=223841;file:///home/jakep/projects/caml/caml/core/cate.py\u001b\\\u001b[2mcate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=621210;file:///home/jakep/projects/caml/caml/core/cate.py#849\u001b\\\u001b[2m849\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> Inidividual Estimator RScores: <span style=\"font-weight: bold\">{</span><span style=\"color: #008000; text-decoration-color: #008000\">'LinearDML'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8540660306283655</span>,           <a href=\"file:///home/jakep/projects/caml/caml/core/cate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/cate.py#850\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">850</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'NonParamDML'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8670505743586602</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DML-Lasso3d'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8302071071188233</span>,      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'CausalForestDML'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8518549197584651</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'XLearner'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8706935718630026</span>,     <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008000; text-decoration-color: #008000\">'DomainAdaptationLearner'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8712209354522116</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'SLearner'</span>:                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8732171083130185</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'TLearner'</span>: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.871951430965139</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'DRLearner'</span>:            <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.8695611614338807</span><span style=\"font-weight: bold\">}</span>                                                        <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m Inidividual Estimator RScores: \u001b[1m{\u001b[0m\u001b[32m'LinearDML'\u001b[0m: \u001b[1;36m0.8540660306283655\u001b[0m,           \u001b]8;id=408668;file:///home/jakep/projects/caml/caml/core/cate.py\u001b\\\u001b[2mcate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=308437;file:///home/jakep/projects/caml/caml/core/cate.py#850\u001b\\\u001b[2m850\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'NonParamDML'\u001b[0m: \u001b[1;36m0.8670505743586602\u001b[0m, \u001b[32m'DML-Lasso3d'\u001b[0m: \u001b[1;36m0.8302071071188233\u001b[0m,      \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'CausalForestDML'\u001b[0m: \u001b[1;36m0.8518549197584651\u001b[0m, \u001b[32m'XLearner'\u001b[0m: \u001b[1;36m0.8706935718630026\u001b[0m,     \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[32m'DomainAdaptationLearner'\u001b[0m: \u001b[1;36m0.8712209354522116\u001b[0m, \u001b[32m'SLearner'\u001b[0m:                 \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.8732171083130185\u001b[0m, \u001b[32m'TLearner'\u001b[0m: \u001b[1;36m0.871951430965139\u001b[0m, \u001b[32m'DRLearner'\u001b[0m:            \u001b[2m           \u001b[0m\n",
       "\u001b[2;36m                    \u001b[0m         \u001b[1;36m0.8695611614338807\u001b[0m\u001b[1m}\u001b[0m                                                        \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> The best estimator is greater than the ensemble estimator. Returning that  <a href=\"file:///home/jakep/projects/caml/caml/core/cate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/cate.py#859\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">859</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         individual estimator.                                                      <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m The best estimator is greater than the ensemble estimator. Returning that  \u001b]8;id=181828;file:///home/jakep/projects/caml/caml/core/cate.py\u001b\\\u001b[2mcate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=733904;file:///home/jakep/projects/caml/caml/core/cate.py#859\u001b\\\u001b[2m859\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         individual estimator.                                                      \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "caml.fit_validator(\n",
    "    subset_cate_models=[\n",
    "        \"LinearDML\",\n",
    "        \"NonParamDML\",\n",
    "        \"DML-Lasso3d\",\n",
    "        \"CausalForestDML\",\n",
    "        \"XLearner\",\n",
    "        \"DomainAdaptationLearner\",\n",
    "        \"SLearner\",\n",
    "        \"TLearner\",\n",
    "        \"DRLearner\",\n",
    "    ],\n",
    "    rscorer_kwargs={},\n",
    "    use_ray=True,\n",
    "    ray_remote_func_options_kwargs={},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> The validation estimator has been fit and will be returned.                <a href=\"file:///home/jakep/projects/caml/caml/core/_base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/_base.py#32\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">32</span></a>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m The validation estimator has been fit and will be returned.                \u001b]8;id=241962;file:///home/jakep/projects/caml/caml/core/_base.py\u001b\\\u001b[2m_base.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=914494;file:///home/jakep/projects/caml/caml/core/_base.py#32\u001b\\\u001b[2m32\u001b[0m\u001b]8;;\u001b\\\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<econml.metalearners._metalearners.SLearner at 0x7fb9ddf8ce50>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml.validation_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CATE Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">[08/24/24 19:20:25] </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> All validation results suggest that the model has found statistically      <a href=\"file:///home/jakep/projects/caml/caml/core/cate.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">cate.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/cate.py#396\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">396</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         significant heterogeneity.                                                 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m[08/24/24 19:20:25]\u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m All validation results suggest that the model has found statistically      \u001b]8;id=340941;file:///home/jakep/projects/caml/caml/core/cate.py\u001b\\\u001b[2mcate.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=861175;file:///home/jakep/projects/caml/caml/core/cate.py#396\u001b\\\u001b[2m396\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         significant heterogeneity.                                                 \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   treatment  blp_est  blp_se  blp_pval  qini_est  qini_se  qini_pval  autoc_est  autoc_se  autoc_pval  cal_r_squared\n",
      "0          1    1.007    0.01       0.0     4.777    0.363        0.0     12.565     1.055         0.0          0.978\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "validation_results = caml.validate(estimator=None, print_full_report=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refit best estimator on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "caml.fit_final()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> The final estimator has been fit on the entire dataset and will be         <a href=\"file:///home/jakep/projects/caml/caml/core/_base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/_base.py#42\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         returned.                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m The final estimator has been fit on the entire dataset and will be         \u001b]8;id=276796;file:///home/jakep/projects/caml/caml/core/_base.py\u001b\\\u001b[2m_base.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=916541;file:///home/jakep/projects/caml/caml/core/_base.py#42\u001b\\\u001b[2m42\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         returned.                                                                  \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<econml.metalearners._metalearners.SLearner at 0x7fb939e57a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml.final_estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict CATEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           X1        X2        X3        X4        X5        X6        X7  \\\n",
      "0   -0.246181 -1.274200 -0.950982 -0.705522  0.132961  0.814546  1.883581   \n",
      "1    1.948685  0.261073 -2.629703  0.005834 -0.352464  1.072183 -1.093752   \n",
      "2    0.615300  1.659502 -0.099698  0.817223  0.106429  0.114220  0.564847   \n",
      "3    0.887987 -0.017229  1.593608  0.885682  0.409091  1.043431  0.394262   \n",
      "4   -0.457293 -0.296993 -0.979472  0.810209 -1.433864 -1.018180 -0.199094   \n",
      "..        ...       ...       ...       ...       ...       ...       ...   \n",
      "995  0.147637 -0.955530  0.393667 -0.281182  0.626307 -0.422972 -2.630214   \n",
      "996  1.310743  1.540612  0.637802  0.060178  0.820293  0.767454  0.222449   \n",
      "997 -0.552284  0.074332  0.477756  0.984183 -0.469270 -1.480282  1.335541   \n",
      "998 -0.241476 -0.174939  1.670931 -1.424675  0.311019  1.019852  0.683656   \n",
      "999  1.204780 -0.018272  0.642867 -1.025366  1.644496 -0.102401  1.113809   \n",
      "\n",
      "           X8        X9       X10  ...       X46       X47       X48  \\\n",
      "0   -0.188843  0.632661 -0.023047  ...  0.869625  0.113842  0.095730   \n",
      "1   -0.427244 -2.305874  0.670639  ...  0.412904 -0.120547 -1.460580   \n",
      "2    0.073989 -0.303836  1.339558  ... -1.870271  1.196852 -2.239998   \n",
      "3   -1.698353  0.558425  0.933242  ... -0.562967 -0.608498 -1.515747   \n",
      "4   -0.996772 -0.402186  2.204502  ...  0.549883  0.295039 -0.636527   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "995  0.163528  1.020511 -0.364770  ...  1.335647 -0.485012  0.668727   \n",
      "996 -0.112176 -0.966916  0.391996  ...  0.842328  0.061312 -0.808946   \n",
      "997  1.114310  0.860029  0.455822  ...  0.136010  1.132567  0.304348   \n",
      "998  0.478273  0.286818  0.141497  ...  0.059641 -0.371086  0.713261   \n",
      "999 -1.072203 -0.491797  1.005130  ...  0.429373  0.735366  0.639385   \n",
      "\n",
      "          X49       X50          y    d  true_cates  uuid  cate_predictions  \n",
      "0    0.716684 -0.302220  22.787988  1.0   19.738300     0         20.160495  \n",
      "1    0.383900  0.445292   4.540934  0.0   11.616198     1          9.236186  \n",
      "2    0.411400 -1.889548   2.802615  0.0   19.294353     2         18.794950  \n",
      "3    0.491019 -0.125751  26.132472  1.0   24.358482     3         23.444381  \n",
      "4   -0.490393 -1.062060  -3.520146  1.0   -6.477713     4         -5.465768  \n",
      "..        ...       ...        ...  ...         ...   ...               ...  \n",
      "995 -1.590170  1.036452  30.132903  1.0   27.992916   995         27.745743  \n",
      "996  0.333095  1.076270   2.968386  0.0   31.238688   996         30.529176  \n",
      "997  1.735459  0.335995  13.280481  1.0    9.661809   997         10.275403  \n",
      "998 -0.126430  0.164833  22.678137  1.0   22.717554   998         21.935939  \n",
      "999  1.897215  1.053077   2.790112  0.0   45.029181   999         43.085561  \n",
      "\n",
      "[1000 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "## \"Out of sample\" predictions\n",
    "\n",
    "df_predictions = caml.predict(\n",
    "    out_of_sample_df=df,\n",
    "    out_of_sample_uuid=\"uuid\",\n",
    "    return_predictions=False,\n",
    "    join_predictions=True,\n",
    ")\n",
    "\n",
    "if df_backend == \"pyspark\":\n",
    "    df_predictions.show()\n",
    "else:\n",
    "    print(df_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X46</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>true_cates</th>\n",
       "      <th>uuid</th>\n",
       "      <th>cate_predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.246181</td>\n",
       "      <td>-1.274200</td>\n",
       "      <td>-0.950982</td>\n",
       "      <td>-0.705522</td>\n",
       "      <td>0.132961</td>\n",
       "      <td>0.814546</td>\n",
       "      <td>1.883581</td>\n",
       "      <td>-0.188843</td>\n",
       "      <td>0.632661</td>\n",
       "      <td>-0.023047</td>\n",
       "      <td>...</td>\n",
       "      <td>0.869625</td>\n",
       "      <td>0.113842</td>\n",
       "      <td>0.095730</td>\n",
       "      <td>0.716684</td>\n",
       "      <td>-0.302220</td>\n",
       "      <td>22.787988</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19.738300</td>\n",
       "      <td>0</td>\n",
       "      <td>20.160495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.948685</td>\n",
       "      <td>0.261073</td>\n",
       "      <td>-2.629703</td>\n",
       "      <td>0.005834</td>\n",
       "      <td>-0.352464</td>\n",
       "      <td>1.072183</td>\n",
       "      <td>-1.093752</td>\n",
       "      <td>-0.427244</td>\n",
       "      <td>-2.305874</td>\n",
       "      <td>0.670639</td>\n",
       "      <td>...</td>\n",
       "      <td>0.412904</td>\n",
       "      <td>-0.120547</td>\n",
       "      <td>-1.460580</td>\n",
       "      <td>0.383900</td>\n",
       "      <td>0.445292</td>\n",
       "      <td>4.540934</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11.616198</td>\n",
       "      <td>1</td>\n",
       "      <td>9.236186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.615300</td>\n",
       "      <td>1.659502</td>\n",
       "      <td>-0.099698</td>\n",
       "      <td>0.817223</td>\n",
       "      <td>0.106429</td>\n",
       "      <td>0.114220</td>\n",
       "      <td>0.564847</td>\n",
       "      <td>0.073989</td>\n",
       "      <td>-0.303836</td>\n",
       "      <td>1.339558</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.870271</td>\n",
       "      <td>1.196852</td>\n",
       "      <td>-2.239998</td>\n",
       "      <td>0.411400</td>\n",
       "      <td>-1.889548</td>\n",
       "      <td>2.802615</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.294353</td>\n",
       "      <td>2</td>\n",
       "      <td>18.794950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.887987</td>\n",
       "      <td>-0.017229</td>\n",
       "      <td>1.593608</td>\n",
       "      <td>0.885682</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1.043431</td>\n",
       "      <td>0.394262</td>\n",
       "      <td>-1.698353</td>\n",
       "      <td>0.558425</td>\n",
       "      <td>0.933242</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.562967</td>\n",
       "      <td>-0.608498</td>\n",
       "      <td>-1.515747</td>\n",
       "      <td>0.491019</td>\n",
       "      <td>-0.125751</td>\n",
       "      <td>26.132472</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.358482</td>\n",
       "      <td>3</td>\n",
       "      <td>23.444381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.457293</td>\n",
       "      <td>-0.296993</td>\n",
       "      <td>-0.979472</td>\n",
       "      <td>0.810209</td>\n",
       "      <td>-1.433864</td>\n",
       "      <td>-1.018180</td>\n",
       "      <td>-0.199094</td>\n",
       "      <td>-0.996772</td>\n",
       "      <td>-0.402186</td>\n",
       "      <td>2.204502</td>\n",
       "      <td>...</td>\n",
       "      <td>0.549883</td>\n",
       "      <td>0.295039</td>\n",
       "      <td>-0.636527</td>\n",
       "      <td>-0.490393</td>\n",
       "      <td>-1.062060</td>\n",
       "      <td>-3.520146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-6.477713</td>\n",
       "      <td>4</td>\n",
       "      <td>-5.465768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>0.147637</td>\n",
       "      <td>-0.955530</td>\n",
       "      <td>0.393667</td>\n",
       "      <td>-0.281182</td>\n",
       "      <td>0.626307</td>\n",
       "      <td>-0.422972</td>\n",
       "      <td>-2.630214</td>\n",
       "      <td>0.163528</td>\n",
       "      <td>1.020511</td>\n",
       "      <td>-0.364770</td>\n",
       "      <td>...</td>\n",
       "      <td>1.335647</td>\n",
       "      <td>-0.485012</td>\n",
       "      <td>0.668727</td>\n",
       "      <td>-1.590170</td>\n",
       "      <td>1.036452</td>\n",
       "      <td>30.132903</td>\n",
       "      <td>1.0</td>\n",
       "      <td>27.992916</td>\n",
       "      <td>995</td>\n",
       "      <td>27.745743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>1.310743</td>\n",
       "      <td>1.540612</td>\n",
       "      <td>0.637802</td>\n",
       "      <td>0.060178</td>\n",
       "      <td>0.820293</td>\n",
       "      <td>0.767454</td>\n",
       "      <td>0.222449</td>\n",
       "      <td>-0.112176</td>\n",
       "      <td>-0.966916</td>\n",
       "      <td>0.391996</td>\n",
       "      <td>...</td>\n",
       "      <td>0.842328</td>\n",
       "      <td>0.061312</td>\n",
       "      <td>-0.808946</td>\n",
       "      <td>0.333095</td>\n",
       "      <td>1.076270</td>\n",
       "      <td>2.968386</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31.238688</td>\n",
       "      <td>996</td>\n",
       "      <td>30.529176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>-0.552284</td>\n",
       "      <td>0.074332</td>\n",
       "      <td>0.477756</td>\n",
       "      <td>0.984183</td>\n",
       "      <td>-0.469270</td>\n",
       "      <td>-1.480282</td>\n",
       "      <td>1.335541</td>\n",
       "      <td>1.114310</td>\n",
       "      <td>0.860029</td>\n",
       "      <td>0.455822</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136010</td>\n",
       "      <td>1.132567</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>1.735459</td>\n",
       "      <td>0.335995</td>\n",
       "      <td>13.280481</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.661809</td>\n",
       "      <td>997</td>\n",
       "      <td>10.275403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.241476</td>\n",
       "      <td>-0.174939</td>\n",
       "      <td>1.670931</td>\n",
       "      <td>-1.424675</td>\n",
       "      <td>0.311019</td>\n",
       "      <td>1.019852</td>\n",
       "      <td>0.683656</td>\n",
       "      <td>0.478273</td>\n",
       "      <td>0.286818</td>\n",
       "      <td>0.141497</td>\n",
       "      <td>...</td>\n",
       "      <td>0.059641</td>\n",
       "      <td>-0.371086</td>\n",
       "      <td>0.713261</td>\n",
       "      <td>-0.126430</td>\n",
       "      <td>0.164833</td>\n",
       "      <td>22.678137</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.717554</td>\n",
       "      <td>998</td>\n",
       "      <td>21.935939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1.204780</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>0.642867</td>\n",
       "      <td>-1.025366</td>\n",
       "      <td>1.644496</td>\n",
       "      <td>-0.102401</td>\n",
       "      <td>1.113809</td>\n",
       "      <td>-1.072203</td>\n",
       "      <td>-0.491797</td>\n",
       "      <td>1.005130</td>\n",
       "      <td>...</td>\n",
       "      <td>0.429373</td>\n",
       "      <td>0.735366</td>\n",
       "      <td>0.639385</td>\n",
       "      <td>1.897215</td>\n",
       "      <td>1.053077</td>\n",
       "      <td>2.790112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45.029181</td>\n",
       "      <td>999</td>\n",
       "      <td>43.085561</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 55 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0   -0.246181 -1.274200 -0.950982 -0.705522  0.132961  0.814546  1.883581   \n",
       "1    1.948685  0.261073 -2.629703  0.005834 -0.352464  1.072183 -1.093752   \n",
       "2    0.615300  1.659502 -0.099698  0.817223  0.106429  0.114220  0.564847   \n",
       "3    0.887987 -0.017229  1.593608  0.885682  0.409091  1.043431  0.394262   \n",
       "4   -0.457293 -0.296993 -0.979472  0.810209 -1.433864 -1.018180 -0.199094   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  0.147637 -0.955530  0.393667 -0.281182  0.626307 -0.422972 -2.630214   \n",
       "996  1.310743  1.540612  0.637802  0.060178  0.820293  0.767454  0.222449   \n",
       "997 -0.552284  0.074332  0.477756  0.984183 -0.469270 -1.480282  1.335541   \n",
       "998 -0.241476 -0.174939  1.670931 -1.424675  0.311019  1.019852  0.683656   \n",
       "999  1.204780 -0.018272  0.642867 -1.025366  1.644496 -0.102401  1.113809   \n",
       "\n",
       "           X8        X9       X10  ...       X46       X47       X48  \\\n",
       "0   -0.188843  0.632661 -0.023047  ...  0.869625  0.113842  0.095730   \n",
       "1   -0.427244 -2.305874  0.670639  ...  0.412904 -0.120547 -1.460580   \n",
       "2    0.073989 -0.303836  1.339558  ... -1.870271  1.196852 -2.239998   \n",
       "3   -1.698353  0.558425  0.933242  ... -0.562967 -0.608498 -1.515747   \n",
       "4   -0.996772 -0.402186  2.204502  ...  0.549883  0.295039 -0.636527   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.163528  1.020511 -0.364770  ...  1.335647 -0.485012  0.668727   \n",
       "996 -0.112176 -0.966916  0.391996  ...  0.842328  0.061312 -0.808946   \n",
       "997  1.114310  0.860029  0.455822  ...  0.136010  1.132567  0.304348   \n",
       "998  0.478273  0.286818  0.141497  ...  0.059641 -0.371086  0.713261   \n",
       "999 -1.072203 -0.491797  1.005130  ...  0.429373  0.735366  0.639385   \n",
       "\n",
       "          X49       X50          y    d  true_cates  uuid  cate_predictions  \n",
       "0    0.716684 -0.302220  22.787988  1.0   19.738300     0         20.160495  \n",
       "1    0.383900  0.445292   4.540934  0.0   11.616198     1          9.236186  \n",
       "2    0.411400 -1.889548   2.802615  0.0   19.294353     2         18.794950  \n",
       "3    0.491019 -0.125751  26.132472  1.0   24.358482     3         23.444381  \n",
       "4   -0.490393 -1.062060  -3.520146  1.0   -6.477713     4         -5.465768  \n",
       "..        ...       ...        ...  ...         ...   ...               ...  \n",
       "995 -1.590170  1.036452  30.132903  1.0   27.992916   995         27.745743  \n",
       "996  0.333095  1.076270   2.968386  0.0   31.238688   996         30.529176  \n",
       "997  1.735459  0.335995  13.280481  1.0    9.661809   997         10.275403  \n",
       "998 -0.126430  0.164833  22.678137  1.0   22.717554   998         21.935939  \n",
       "999  1.897215  1.053077   2.790112  0.0   45.029181   999         43.085561  \n",
       "\n",
       "[1000 rows x 55 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Append to internal dataframe\n",
    "\n",
    "caml.predict(\n",
    "    out_of_sample_df=None,\n",
    "    out_of_sample_uuid=None,\n",
    "    join_predictions=True,\n",
    "    return_predictions=False,\n",
    ")\n",
    "\n",
    "caml.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CATE Rank Ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>true_cates</th>\n",
       "      <th>uuid</th>\n",
       "      <th>cate_predictions</th>\n",
       "      <th>cate_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098873</td>\n",
       "      <td>-2.047471</td>\n",
       "      <td>-0.919803</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>2.857811</td>\n",
       "      <td>0.673014</td>\n",
       "      <td>-0.801269</td>\n",
       "      <td>-1.018265</td>\n",
       "      <td>1.751302</td>\n",
       "      <td>1.135212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>-0.461712</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>65.924937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.330278</td>\n",
       "      <td>277</td>\n",
       "      <td>63.617159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.087877</td>\n",
       "      <td>0.346426</td>\n",
       "      <td>-0.691883</td>\n",
       "      <td>3.506982</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.546016</td>\n",
       "      <td>-0.178966</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.384579</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.872701</td>\n",
       "      <td>-0.473291</td>\n",
       "      <td>1.284942</td>\n",
       "      <td>0.050942</td>\n",
       "      <td>2.401395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.192159</td>\n",
       "      <td>196</td>\n",
       "      <td>60.421574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.135132</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>-0.735048</td>\n",
       "      <td>-0.040662</td>\n",
       "      <td>2.721447</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>-0.818205</td>\n",
       "      <td>1.717546</td>\n",
       "      <td>0.736462</td>\n",
       "      <td>-0.617422</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488313</td>\n",
       "      <td>-1.017432</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>62.921279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.048645</td>\n",
       "      <td>268</td>\n",
       "      <td>59.827181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.419552</td>\n",
       "      <td>1.498328</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>-0.477370</td>\n",
       "      <td>2.498186</td>\n",
       "      <td>-0.771486</td>\n",
       "      <td>-0.507519</td>\n",
       "      <td>-0.954831</td>\n",
       "      <td>-0.606199</td>\n",
       "      <td>-1.123812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639620</td>\n",
       "      <td>-0.315770</td>\n",
       "      <td>-1.615912</td>\n",
       "      <td>-0.327509</td>\n",
       "      <td>62.379173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.313053</td>\n",
       "      <td>429</td>\n",
       "      <td>58.935331</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.037186</td>\n",
       "      <td>-0.026512</td>\n",
       "      <td>-1.291829</td>\n",
       "      <td>-1.071286</td>\n",
       "      <td>2.573731</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>-1.802170</td>\n",
       "      <td>-1.649797</td>\n",
       "      <td>-0.169826</td>\n",
       "      <td>-0.715459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118501</td>\n",
       "      <td>0.805946</td>\n",
       "      <td>0.935876</td>\n",
       "      <td>0.432677</td>\n",
       "      <td>61.545043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.577071</td>\n",
       "      <td>815</td>\n",
       "      <td>58.648805</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.100824</td>\n",
       "      <td>0.315690</td>\n",
       "      <td>-0.673006</td>\n",
       "      <td>0.699648</td>\n",
       "      <td>-2.397660</td>\n",
       "      <td>1.625240</td>\n",
       "      <td>1.397913</td>\n",
       "      <td>0.642165</td>\n",
       "      <td>0.703181</td>\n",
       "      <td>0.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288494</td>\n",
       "      <td>1.224889</td>\n",
       "      <td>0.987245</td>\n",
       "      <td>1.338624</td>\n",
       "      <td>-18.309803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-22.603876</td>\n",
       "      <td>791</td>\n",
       "      <td>-22.041368</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.752559</td>\n",
       "      <td>1.092341</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>-2.481522</td>\n",
       "      <td>1.291770</td>\n",
       "      <td>-1.357008</td>\n",
       "      <td>1.566003</td>\n",
       "      <td>-1.014990</td>\n",
       "      <td>1.185446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404063</td>\n",
       "      <td>0.317593</td>\n",
       "      <td>1.428591</td>\n",
       "      <td>-1.279750</td>\n",
       "      <td>-20.906900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-24.007043</td>\n",
       "      <td>528</td>\n",
       "      <td>-22.153801</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.973172</td>\n",
       "      <td>1.723508</td>\n",
       "      <td>1.155148</td>\n",
       "      <td>-1.562033</td>\n",
       "      <td>-2.345816</td>\n",
       "      <td>-0.795644</td>\n",
       "      <td>-0.147862</td>\n",
       "      <td>0.401086</td>\n",
       "      <td>-1.754101</td>\n",
       "      <td>0.257217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>-0.195044</td>\n",
       "      <td>-0.002387</td>\n",
       "      <td>0.143260</td>\n",
       "      <td>4.570123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.736413</td>\n",
       "      <td>382</td>\n",
       "      <td>-23.204234</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-2.454188</td>\n",
       "      <td>-0.604648</td>\n",
       "      <td>-0.354949</td>\n",
       "      <td>-2.712966</td>\n",
       "      <td>0.827282</td>\n",
       "      <td>-0.779570</td>\n",
       "      <td>0.714476</td>\n",
       "      <td>-0.572021</td>\n",
       "      <td>-1.442043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236065</td>\n",
       "      <td>0.865011</td>\n",
       "      <td>3.110486</td>\n",
       "      <td>1.306388</td>\n",
       "      <td>-26.432933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.879548</td>\n",
       "      <td>624</td>\n",
       "      <td>-23.610627</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.117951</td>\n",
       "      <td>-0.835019</td>\n",
       "      <td>1.192294</td>\n",
       "      <td>0.836876</td>\n",
       "      <td>-2.742192</td>\n",
       "      <td>0.819813</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.540686</td>\n",
       "      <td>0.615511</td>\n",
       "      <td>-0.745564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530729</td>\n",
       "      <td>-1.985961</td>\n",
       "      <td>-0.736775</td>\n",
       "      <td>0.832998</td>\n",
       "      <td>-27.188855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-28.368560</td>\n",
       "      <td>649</td>\n",
       "      <td>-29.255976</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.098873 -2.047471 -0.919803  0.566395  2.857811  0.673014 -0.801269   \n",
       "1    0.000555  0.087877  0.346426 -0.691883  3.506982  0.129047  0.546016   \n",
       "2   -1.135132  0.306710 -0.735048 -0.040662  2.721447  0.029122 -0.818205   \n",
       "3    0.419552  1.498328 -0.001713 -0.477370  2.498186 -0.771486 -0.507519   \n",
       "4    1.037186 -0.026512 -1.291829 -1.071286  2.573731 -0.042306 -1.802170   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  1.100824  0.315690 -0.673006  0.699648 -2.397660  1.625240  1.397913   \n",
       "996 -0.752559  1.092341 -0.421944  0.147824 -2.481522  1.291770 -1.357008   \n",
       "997  1.973172  1.723508  1.155148 -1.562033 -2.345816 -0.795644 -0.147862   \n",
       "998 -0.032742 -2.454188 -0.604648 -0.354949 -2.712966  0.827282 -0.779570   \n",
       "999 -0.117951 -0.835019  1.192294  0.836876 -2.742192  0.819813  0.008420   \n",
       "\n",
       "           X8        X9       X10  ...       X47       X48       X49  \\\n",
       "0   -1.018265  1.751302  1.135212  ...  0.036920 -0.461712  0.006068   \n",
       "1   -0.178966  0.632143  0.384579  ... -1.872701 -0.473291  1.284942   \n",
       "2    1.717546  0.736462 -0.617422  ...  1.488313 -1.017432  0.965691   \n",
       "3   -0.954831 -0.606199 -1.123812  ...  0.639620 -0.315770 -1.615912   \n",
       "4   -1.649797 -0.169826 -0.715459  ... -0.118501  0.805946  0.935876   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.642165  0.703181  0.408108  ... -0.288494  1.224889  0.987245   \n",
       "996  1.566003 -1.014990  1.185446  ... -0.404063  0.317593  1.428591   \n",
       "997  0.401086 -1.754101  0.257217  ...  0.595469 -0.195044 -0.002387   \n",
       "998  0.714476 -0.572021 -1.442043  ... -0.236065  0.865011  3.110486   \n",
       "999  0.540686  0.615511 -0.745564  ...  0.530729 -1.985961 -0.736775   \n",
       "\n",
       "          X50          y    d  true_cates  uuid  cate_predictions  \\\n",
       "0    0.255008  65.924937  1.0   65.330278   277         63.617159   \n",
       "1    0.050942   2.401395  0.0   76.192159   196         60.421574   \n",
       "2    0.276485  62.921279  1.0   63.048645   268         59.827181   \n",
       "3   -0.327509  62.379173  1.0   59.313053   429         58.935331   \n",
       "4    0.432677  61.545043  1.0   60.577071   815         58.648805   \n",
       "..        ...        ...  ...         ...   ...               ...   \n",
       "995  1.338624 -18.309803  1.0  -22.603876   791        -22.041368   \n",
       "996 -1.279750 -20.906900  1.0  -24.007043   528        -22.153801   \n",
       "997  0.143260   4.570123  0.0  -21.736413   382        -23.204234   \n",
       "998  1.306388 -26.432933  1.0  -27.879548   624        -23.610627   \n",
       "999  0.832998 -27.188855  1.0  -28.368560   649        -29.255976   \n",
       "\n",
       "     cate_ranking  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               3  \n",
       "4               4  \n",
       "..            ...  \n",
       "995           995  \n",
       "996           996  \n",
       "997           997  \n",
       "998           998  \n",
       "999           999  \n",
       "\n",
       "[1000 rows x 56 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## \"Out of sample\" predictions\n",
    "\n",
    "df_rank_ordered = caml.rank_order(\n",
    "    out_of_sample_df=df_predictions, return_rank_order=False, join_rank_order=True\n",
    ")\n",
    "\n",
    "df_rank_ordered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>true_cates</th>\n",
       "      <th>uuid</th>\n",
       "      <th>cate_predictions</th>\n",
       "      <th>cate_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098873</td>\n",
       "      <td>-2.047471</td>\n",
       "      <td>-0.919803</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>2.857811</td>\n",
       "      <td>0.673014</td>\n",
       "      <td>-0.801269</td>\n",
       "      <td>-1.018265</td>\n",
       "      <td>1.751302</td>\n",
       "      <td>1.135212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>-0.461712</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>65.924937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.330278</td>\n",
       "      <td>277</td>\n",
       "      <td>63.617159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.087877</td>\n",
       "      <td>0.346426</td>\n",
       "      <td>-0.691883</td>\n",
       "      <td>3.506982</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.546016</td>\n",
       "      <td>-0.178966</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.384579</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.872701</td>\n",
       "      <td>-0.473291</td>\n",
       "      <td>1.284942</td>\n",
       "      <td>0.050942</td>\n",
       "      <td>2.401395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.192159</td>\n",
       "      <td>196</td>\n",
       "      <td>60.421574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.135132</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>-0.735048</td>\n",
       "      <td>-0.040662</td>\n",
       "      <td>2.721447</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>-0.818205</td>\n",
       "      <td>1.717546</td>\n",
       "      <td>0.736462</td>\n",
       "      <td>-0.617422</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488313</td>\n",
       "      <td>-1.017432</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>62.921279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.048645</td>\n",
       "      <td>268</td>\n",
       "      <td>59.827181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.419552</td>\n",
       "      <td>1.498328</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>-0.477370</td>\n",
       "      <td>2.498186</td>\n",
       "      <td>-0.771486</td>\n",
       "      <td>-0.507519</td>\n",
       "      <td>-0.954831</td>\n",
       "      <td>-0.606199</td>\n",
       "      <td>-1.123812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639620</td>\n",
       "      <td>-0.315770</td>\n",
       "      <td>-1.615912</td>\n",
       "      <td>-0.327509</td>\n",
       "      <td>62.379173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.313053</td>\n",
       "      <td>429</td>\n",
       "      <td>58.935331</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.037186</td>\n",
       "      <td>-0.026512</td>\n",
       "      <td>-1.291829</td>\n",
       "      <td>-1.071286</td>\n",
       "      <td>2.573731</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>-1.802170</td>\n",
       "      <td>-1.649797</td>\n",
       "      <td>-0.169826</td>\n",
       "      <td>-0.715459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118501</td>\n",
       "      <td>0.805946</td>\n",
       "      <td>0.935876</td>\n",
       "      <td>0.432677</td>\n",
       "      <td>61.545043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.577071</td>\n",
       "      <td>815</td>\n",
       "      <td>58.648805</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.100824</td>\n",
       "      <td>0.315690</td>\n",
       "      <td>-0.673006</td>\n",
       "      <td>0.699648</td>\n",
       "      <td>-2.397660</td>\n",
       "      <td>1.625240</td>\n",
       "      <td>1.397913</td>\n",
       "      <td>0.642165</td>\n",
       "      <td>0.703181</td>\n",
       "      <td>0.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288494</td>\n",
       "      <td>1.224889</td>\n",
       "      <td>0.987245</td>\n",
       "      <td>1.338624</td>\n",
       "      <td>-18.309803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-22.603876</td>\n",
       "      <td>791</td>\n",
       "      <td>-22.041368</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.752559</td>\n",
       "      <td>1.092341</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>-2.481522</td>\n",
       "      <td>1.291770</td>\n",
       "      <td>-1.357008</td>\n",
       "      <td>1.566003</td>\n",
       "      <td>-1.014990</td>\n",
       "      <td>1.185446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404063</td>\n",
       "      <td>0.317593</td>\n",
       "      <td>1.428591</td>\n",
       "      <td>-1.279750</td>\n",
       "      <td>-20.906900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-24.007043</td>\n",
       "      <td>528</td>\n",
       "      <td>-22.153801</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.973172</td>\n",
       "      <td>1.723508</td>\n",
       "      <td>1.155148</td>\n",
       "      <td>-1.562033</td>\n",
       "      <td>-2.345816</td>\n",
       "      <td>-0.795644</td>\n",
       "      <td>-0.147862</td>\n",
       "      <td>0.401086</td>\n",
       "      <td>-1.754101</td>\n",
       "      <td>0.257217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>-0.195044</td>\n",
       "      <td>-0.002387</td>\n",
       "      <td>0.143260</td>\n",
       "      <td>4.570123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.736413</td>\n",
       "      <td>382</td>\n",
       "      <td>-23.204234</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-2.454188</td>\n",
       "      <td>-0.604648</td>\n",
       "      <td>-0.354949</td>\n",
       "      <td>-2.712966</td>\n",
       "      <td>0.827282</td>\n",
       "      <td>-0.779570</td>\n",
       "      <td>0.714476</td>\n",
       "      <td>-0.572021</td>\n",
       "      <td>-1.442043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236065</td>\n",
       "      <td>0.865011</td>\n",
       "      <td>3.110486</td>\n",
       "      <td>1.306388</td>\n",
       "      <td>-26.432933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.879548</td>\n",
       "      <td>624</td>\n",
       "      <td>-23.610627</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.117951</td>\n",
       "      <td>-0.835019</td>\n",
       "      <td>1.192294</td>\n",
       "      <td>0.836876</td>\n",
       "      <td>-2.742192</td>\n",
       "      <td>0.819813</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.540686</td>\n",
       "      <td>0.615511</td>\n",
       "      <td>-0.745564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530729</td>\n",
       "      <td>-1.985961</td>\n",
       "      <td>-0.736775</td>\n",
       "      <td>0.832998</td>\n",
       "      <td>-27.188855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-28.368560</td>\n",
       "      <td>649</td>\n",
       "      <td>-29.255976</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.098873 -2.047471 -0.919803  0.566395  2.857811  0.673014 -0.801269   \n",
       "1    0.000555  0.087877  0.346426 -0.691883  3.506982  0.129047  0.546016   \n",
       "2   -1.135132  0.306710 -0.735048 -0.040662  2.721447  0.029122 -0.818205   \n",
       "3    0.419552  1.498328 -0.001713 -0.477370  2.498186 -0.771486 -0.507519   \n",
       "4    1.037186 -0.026512 -1.291829 -1.071286  2.573731 -0.042306 -1.802170   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  1.100824  0.315690 -0.673006  0.699648 -2.397660  1.625240  1.397913   \n",
       "996 -0.752559  1.092341 -0.421944  0.147824 -2.481522  1.291770 -1.357008   \n",
       "997  1.973172  1.723508  1.155148 -1.562033 -2.345816 -0.795644 -0.147862   \n",
       "998 -0.032742 -2.454188 -0.604648 -0.354949 -2.712966  0.827282 -0.779570   \n",
       "999 -0.117951 -0.835019  1.192294  0.836876 -2.742192  0.819813  0.008420   \n",
       "\n",
       "           X8        X9       X10  ...       X47       X48       X49  \\\n",
       "0   -1.018265  1.751302  1.135212  ...  0.036920 -0.461712  0.006068   \n",
       "1   -0.178966  0.632143  0.384579  ... -1.872701 -0.473291  1.284942   \n",
       "2    1.717546  0.736462 -0.617422  ...  1.488313 -1.017432  0.965691   \n",
       "3   -0.954831 -0.606199 -1.123812  ...  0.639620 -0.315770 -1.615912   \n",
       "4   -1.649797 -0.169826 -0.715459  ... -0.118501  0.805946  0.935876   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.642165  0.703181  0.408108  ... -0.288494  1.224889  0.987245   \n",
       "996  1.566003 -1.014990  1.185446  ... -0.404063  0.317593  1.428591   \n",
       "997  0.401086 -1.754101  0.257217  ...  0.595469 -0.195044 -0.002387   \n",
       "998  0.714476 -0.572021 -1.442043  ... -0.236065  0.865011  3.110486   \n",
       "999  0.540686  0.615511 -0.745564  ...  0.530729 -1.985961 -0.736775   \n",
       "\n",
       "          X50          y    d  true_cates  uuid  cate_predictions  \\\n",
       "0    0.255008  65.924937  1.0   65.330278   277         63.617159   \n",
       "1    0.050942   2.401395  0.0   76.192159   196         60.421574   \n",
       "2    0.276485  62.921279  1.0   63.048645   268         59.827181   \n",
       "3   -0.327509  62.379173  1.0   59.313053   429         58.935331   \n",
       "4    0.432677  61.545043  1.0   60.577071   815         58.648805   \n",
       "..        ...        ...  ...         ...   ...               ...   \n",
       "995  1.338624 -18.309803  1.0  -22.603876   791        -22.041368   \n",
       "996 -1.279750 -20.906900  1.0  -24.007043   528        -22.153801   \n",
       "997  0.143260   4.570123  0.0  -21.736413   382        -23.204234   \n",
       "998  1.306388 -26.432933  1.0  -27.879548   624        -23.610627   \n",
       "999  0.832998 -27.188855  1.0  -28.368560   649        -29.255976   \n",
       "\n",
       "     cate_ranking  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               3  \n",
       "4               4  \n",
       "..            ...  \n",
       "995           995  \n",
       "996           996  \n",
       "997           997  \n",
       "998           998  \n",
       "999           999  \n",
       "\n",
       "[1000 rows x 56 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Append to internal dataframe\n",
    "\n",
    "caml.rank_order(out_of_sample_df=None, return_rank_order=False, join_rank_order=True)\n",
    "\n",
    "caml.dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CATE Visualization/Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cate_mean</th>\n",
       "      <th>cate_sum</th>\n",
       "      <th>cate_std</th>\n",
       "      <th>cate_min</th>\n",
       "      <th>cate_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.923216</td>\n",
       "      <td>16923.216395</td>\n",
       "      <td>17.060288</td>\n",
       "      <td>-29.255976</td>\n",
       "      <td>63.617159</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cate_mean      cate_sum   cate_std   cate_min   cate_max  count\n",
       "0  16.923216  16923.216395  17.060288 -29.255976  63.617159   1000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_summary = caml.summarize(out_of_sample_df=df_rank_ordered)\n",
    "\n",
    "cate_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cate_mean</th>\n",
       "      <th>cate_sum</th>\n",
       "      <th>cate_std</th>\n",
       "      <th>cate_min</th>\n",
       "      <th>cate_max</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>16.923216</td>\n",
       "      <td>16923.216395</td>\n",
       "      <td>17.060288</td>\n",
       "      <td>-29.255976</td>\n",
       "      <td>63.617159</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cate_mean      cate_sum   cate_std   cate_min   cate_max  count\n",
       "0  16.923216  16923.216395  17.060288 -29.255976  63.617159   1000"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cate_summary = caml.summarize(out_of_sample_df=None)\n",
    "\n",
    "cate_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Access my dataframe and estimator object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X1</th>\n",
       "      <th>X2</th>\n",
       "      <th>X3</th>\n",
       "      <th>X4</th>\n",
       "      <th>X5</th>\n",
       "      <th>X6</th>\n",
       "      <th>X7</th>\n",
       "      <th>X8</th>\n",
       "      <th>X9</th>\n",
       "      <th>X10</th>\n",
       "      <th>...</th>\n",
       "      <th>X47</th>\n",
       "      <th>X48</th>\n",
       "      <th>X49</th>\n",
       "      <th>X50</th>\n",
       "      <th>y</th>\n",
       "      <th>d</th>\n",
       "      <th>true_cates</th>\n",
       "      <th>uuid</th>\n",
       "      <th>cate_predictions</th>\n",
       "      <th>cate_ranking</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.098873</td>\n",
       "      <td>-2.047471</td>\n",
       "      <td>-0.919803</td>\n",
       "      <td>0.566395</td>\n",
       "      <td>2.857811</td>\n",
       "      <td>0.673014</td>\n",
       "      <td>-0.801269</td>\n",
       "      <td>-1.018265</td>\n",
       "      <td>1.751302</td>\n",
       "      <td>1.135212</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036920</td>\n",
       "      <td>-0.461712</td>\n",
       "      <td>0.006068</td>\n",
       "      <td>0.255008</td>\n",
       "      <td>65.924937</td>\n",
       "      <td>1.0</td>\n",
       "      <td>65.330278</td>\n",
       "      <td>277</td>\n",
       "      <td>63.617159</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000555</td>\n",
       "      <td>0.087877</td>\n",
       "      <td>0.346426</td>\n",
       "      <td>-0.691883</td>\n",
       "      <td>3.506982</td>\n",
       "      <td>0.129047</td>\n",
       "      <td>0.546016</td>\n",
       "      <td>-0.178966</td>\n",
       "      <td>0.632143</td>\n",
       "      <td>0.384579</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.872701</td>\n",
       "      <td>-0.473291</td>\n",
       "      <td>1.284942</td>\n",
       "      <td>0.050942</td>\n",
       "      <td>2.401395</td>\n",
       "      <td>0.0</td>\n",
       "      <td>76.192159</td>\n",
       "      <td>196</td>\n",
       "      <td>60.421574</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.135132</td>\n",
       "      <td>0.306710</td>\n",
       "      <td>-0.735048</td>\n",
       "      <td>-0.040662</td>\n",
       "      <td>2.721447</td>\n",
       "      <td>0.029122</td>\n",
       "      <td>-0.818205</td>\n",
       "      <td>1.717546</td>\n",
       "      <td>0.736462</td>\n",
       "      <td>-0.617422</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488313</td>\n",
       "      <td>-1.017432</td>\n",
       "      <td>0.965691</td>\n",
       "      <td>0.276485</td>\n",
       "      <td>62.921279</td>\n",
       "      <td>1.0</td>\n",
       "      <td>63.048645</td>\n",
       "      <td>268</td>\n",
       "      <td>59.827181</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.419552</td>\n",
       "      <td>1.498328</td>\n",
       "      <td>-0.001713</td>\n",
       "      <td>-0.477370</td>\n",
       "      <td>2.498186</td>\n",
       "      <td>-0.771486</td>\n",
       "      <td>-0.507519</td>\n",
       "      <td>-0.954831</td>\n",
       "      <td>-0.606199</td>\n",
       "      <td>-1.123812</td>\n",
       "      <td>...</td>\n",
       "      <td>0.639620</td>\n",
       "      <td>-0.315770</td>\n",
       "      <td>-1.615912</td>\n",
       "      <td>-0.327509</td>\n",
       "      <td>62.379173</td>\n",
       "      <td>1.0</td>\n",
       "      <td>59.313053</td>\n",
       "      <td>429</td>\n",
       "      <td>58.935331</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.037186</td>\n",
       "      <td>-0.026512</td>\n",
       "      <td>-1.291829</td>\n",
       "      <td>-1.071286</td>\n",
       "      <td>2.573731</td>\n",
       "      <td>-0.042306</td>\n",
       "      <td>-1.802170</td>\n",
       "      <td>-1.649797</td>\n",
       "      <td>-0.169826</td>\n",
       "      <td>-0.715459</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.118501</td>\n",
       "      <td>0.805946</td>\n",
       "      <td>0.935876</td>\n",
       "      <td>0.432677</td>\n",
       "      <td>61.545043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>60.577071</td>\n",
       "      <td>815</td>\n",
       "      <td>58.648805</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>1.100824</td>\n",
       "      <td>0.315690</td>\n",
       "      <td>-0.673006</td>\n",
       "      <td>0.699648</td>\n",
       "      <td>-2.397660</td>\n",
       "      <td>1.625240</td>\n",
       "      <td>1.397913</td>\n",
       "      <td>0.642165</td>\n",
       "      <td>0.703181</td>\n",
       "      <td>0.408108</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.288494</td>\n",
       "      <td>1.224889</td>\n",
       "      <td>0.987245</td>\n",
       "      <td>1.338624</td>\n",
       "      <td>-18.309803</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-22.603876</td>\n",
       "      <td>791</td>\n",
       "      <td>-22.041368</td>\n",
       "      <td>995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>-0.752559</td>\n",
       "      <td>1.092341</td>\n",
       "      <td>-0.421944</td>\n",
       "      <td>0.147824</td>\n",
       "      <td>-2.481522</td>\n",
       "      <td>1.291770</td>\n",
       "      <td>-1.357008</td>\n",
       "      <td>1.566003</td>\n",
       "      <td>-1.014990</td>\n",
       "      <td>1.185446</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404063</td>\n",
       "      <td>0.317593</td>\n",
       "      <td>1.428591</td>\n",
       "      <td>-1.279750</td>\n",
       "      <td>-20.906900</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-24.007043</td>\n",
       "      <td>528</td>\n",
       "      <td>-22.153801</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1.973172</td>\n",
       "      <td>1.723508</td>\n",
       "      <td>1.155148</td>\n",
       "      <td>-1.562033</td>\n",
       "      <td>-2.345816</td>\n",
       "      <td>-0.795644</td>\n",
       "      <td>-0.147862</td>\n",
       "      <td>0.401086</td>\n",
       "      <td>-1.754101</td>\n",
       "      <td>0.257217</td>\n",
       "      <td>...</td>\n",
       "      <td>0.595469</td>\n",
       "      <td>-0.195044</td>\n",
       "      <td>-0.002387</td>\n",
       "      <td>0.143260</td>\n",
       "      <td>4.570123</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-21.736413</td>\n",
       "      <td>382</td>\n",
       "      <td>-23.204234</td>\n",
       "      <td>997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-0.032742</td>\n",
       "      <td>-2.454188</td>\n",
       "      <td>-0.604648</td>\n",
       "      <td>-0.354949</td>\n",
       "      <td>-2.712966</td>\n",
       "      <td>0.827282</td>\n",
       "      <td>-0.779570</td>\n",
       "      <td>0.714476</td>\n",
       "      <td>-0.572021</td>\n",
       "      <td>-1.442043</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.236065</td>\n",
       "      <td>0.865011</td>\n",
       "      <td>3.110486</td>\n",
       "      <td>1.306388</td>\n",
       "      <td>-26.432933</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-27.879548</td>\n",
       "      <td>624</td>\n",
       "      <td>-23.610627</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-0.117951</td>\n",
       "      <td>-0.835019</td>\n",
       "      <td>1.192294</td>\n",
       "      <td>0.836876</td>\n",
       "      <td>-2.742192</td>\n",
       "      <td>0.819813</td>\n",
       "      <td>0.008420</td>\n",
       "      <td>0.540686</td>\n",
       "      <td>0.615511</td>\n",
       "      <td>-0.745564</td>\n",
       "      <td>...</td>\n",
       "      <td>0.530729</td>\n",
       "      <td>-1.985961</td>\n",
       "      <td>-0.736775</td>\n",
       "      <td>0.832998</td>\n",
       "      <td>-27.188855</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-28.368560</td>\n",
       "      <td>649</td>\n",
       "      <td>-29.255976</td>\n",
       "      <td>999</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows Ã— 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           X1        X2        X3        X4        X5        X6        X7  \\\n",
       "0    0.098873 -2.047471 -0.919803  0.566395  2.857811  0.673014 -0.801269   \n",
       "1    0.000555  0.087877  0.346426 -0.691883  3.506982  0.129047  0.546016   \n",
       "2   -1.135132  0.306710 -0.735048 -0.040662  2.721447  0.029122 -0.818205   \n",
       "3    0.419552  1.498328 -0.001713 -0.477370  2.498186 -0.771486 -0.507519   \n",
       "4    1.037186 -0.026512 -1.291829 -1.071286  2.573731 -0.042306 -1.802170   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "995  1.100824  0.315690 -0.673006  0.699648 -2.397660  1.625240  1.397913   \n",
       "996 -0.752559  1.092341 -0.421944  0.147824 -2.481522  1.291770 -1.357008   \n",
       "997  1.973172  1.723508  1.155148 -1.562033 -2.345816 -0.795644 -0.147862   \n",
       "998 -0.032742 -2.454188 -0.604648 -0.354949 -2.712966  0.827282 -0.779570   \n",
       "999 -0.117951 -0.835019  1.192294  0.836876 -2.742192  0.819813  0.008420   \n",
       "\n",
       "           X8        X9       X10  ...       X47       X48       X49  \\\n",
       "0   -1.018265  1.751302  1.135212  ...  0.036920 -0.461712  0.006068   \n",
       "1   -0.178966  0.632143  0.384579  ... -1.872701 -0.473291  1.284942   \n",
       "2    1.717546  0.736462 -0.617422  ...  1.488313 -1.017432  0.965691   \n",
       "3   -0.954831 -0.606199 -1.123812  ...  0.639620 -0.315770 -1.615912   \n",
       "4   -1.649797 -0.169826 -0.715459  ... -0.118501  0.805946  0.935876   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "995  0.642165  0.703181  0.408108  ... -0.288494  1.224889  0.987245   \n",
       "996  1.566003 -1.014990  1.185446  ... -0.404063  0.317593  1.428591   \n",
       "997  0.401086 -1.754101  0.257217  ...  0.595469 -0.195044 -0.002387   \n",
       "998  0.714476 -0.572021 -1.442043  ... -0.236065  0.865011  3.110486   \n",
       "999  0.540686  0.615511 -0.745564  ...  0.530729 -1.985961 -0.736775   \n",
       "\n",
       "          X50          y    d  true_cates  uuid  cate_predictions  \\\n",
       "0    0.255008  65.924937  1.0   65.330278   277         63.617159   \n",
       "1    0.050942   2.401395  0.0   76.192159   196         60.421574   \n",
       "2    0.276485  62.921279  1.0   63.048645   268         59.827181   \n",
       "3   -0.327509  62.379173  1.0   59.313053   429         58.935331   \n",
       "4    0.432677  61.545043  1.0   60.577071   815         58.648805   \n",
       "..        ...        ...  ...         ...   ...               ...   \n",
       "995  1.338624 -18.309803  1.0  -22.603876   791        -22.041368   \n",
       "996 -1.279750 -20.906900  1.0  -24.007043   528        -22.153801   \n",
       "997  0.143260   4.570123  0.0  -21.736413   382        -23.204234   \n",
       "998  1.306388 -26.432933  1.0  -27.879548   624        -23.610627   \n",
       "999  0.832998 -27.188855  1.0  -28.368560   649        -29.255976   \n",
       "\n",
       "     cate_ranking  \n",
       "0               0  \n",
       "1               1  \n",
       "2               2  \n",
       "3               3  \n",
       "4               4  \n",
       "..            ...  \n",
       "995           995  \n",
       "996           996  \n",
       "997           997  \n",
       "998           998  \n",
       "999           999  \n",
       "\n",
       "[1000 rows x 56 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "caml.dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span><span style=\"color: #008000; text-decoration-color: #008000\">INFO    </span> The final estimator has been fit on the entire dataset and will be         <a href=\"file:///home/jakep/projects/caml/caml/core/_base.py\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">_base.py</span></a><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">:</span><a href=\"file:///home/jakep/projects/caml/caml/core/_base.py#42\" target=\"_blank\"><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">42</span></a>\n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                    </span>         returned.                                                                  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">           </span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m                   \u001b[0m\u001b[2;36m \u001b[0m\u001b[32mINFO    \u001b[0m The final estimator has been fit on the entire dataset and will be         \u001b]8;id=843398;file:///home/jakep/projects/caml/caml/core/_base.py\u001b\\\u001b[2m_base.py\u001b[0m\u001b]8;;\u001b\\\u001b[2m:\u001b[0m\u001b]8;id=493535;file:///home/jakep/projects/caml/caml/core/_base.py#42\u001b\\\u001b[2m42\u001b[0m\u001b]8;;\u001b\\\n",
       "\u001b[2;36m                    \u001b[0m         returned.                                                                  \u001b[2m           \u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<econml.metalearners._metalearners.SLearner object at 0x7fb939e57a60>\n",
      "{'feature_names': ['X0', 'X1', 'X2', 'X3', 'X4', 'X5', 'X6', 'X7', 'X8', 'X9', 'X10', 'X11', 'X12', 'X13', 'X14', 'X15', 'X16', 'X17', 'X18', 'X19', 'X20', 'X21', 'X22', 'X23', 'X24', 'X25', 'X26', 'X27', 'X28', 'X29', 'X30', 'X31', 'X32', 'X33', 'X34', 'X35', 'X36', 'X37', 'X38', 'X39', 'X40', 'X41', 'X42', 'X43', 'X44', 'X45', 'X46', 'X47', 'X48', 'X49'], 'output_names': ['Y0'], 'treatment_names': ['T0_1.0']}\n"
     ]
    }
   ],
   "source": [
    "from econml.score import EnsembleCateEstimator\n",
    "\n",
    "# Use this estimator object as pickled object for optimized inference\n",
    "final_estimator = caml.final_estimator\n",
    "\n",
    "if isinstance(final_estimator, EnsembleCateEstimator):\n",
    "    for model in final_estimator._cate_models:\n",
    "        print(model)\n",
    "        print(model._input_names)\n",
    "else:\n",
    "    print(final_estimator)\n",
    "    print(final_estimator._input_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
